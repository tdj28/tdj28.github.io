[{"content":"In this post, we explore Bayesian probabilities through the lens of the accuracies of medical tests. We show how Bayesian priors can help refine our projections, and how to do this accurately.\nWarning This post is for a mathematical discussion, not a medical discussion. The intent is to inform the reader about how probability works in relation to tests that can have false positives/negatives. The author has no medical training nor is this post intended to provide medical advice. It is hoped that the reader will better understand likelihoods of test results in the context of the mathematics, not make medical decisions based on this post. Person X Imagine Person X, who is exhibiting the beginnings of flu-like symptoms. They have heard on the news that COVID-19 is spreading locally. Person X purchases an over-the-counter COVID-19 rapid test, and they get a negative result. They are relieved that it is not COVID-19, and head off to their job. They think to themselves, yes, maybe there is a small chance I\u0026rsquo;ll give a coworker the flu, but I can\u0026rsquo;t take a sick day unless I really need it, and this isn\u0026rsquo;t COVID.\nUnfortunately, Person X isn\u0026rsquo;t fully aware that medical tests can have false positives and false negatives, and that the over-the-counter tests tend to be somewhat less accurate than the more formal tests. The complexities of public education on these matters is far beyond the scope of this post, but if you were responsible for public health, you must make some tough choices in educating the public about the possible false negative or false positive results from testing.\nOn the one hand, if you push a lot of cautionary information to the public, a number of individuals will form an opinion that these tests aren\u0026rsquo;t 100% accurate so why bother, and the percentage of people who will make an effort to tests themselves will likely go down. On the other hand, by not emphasizing the potential inaccuracies in these tests, you may get a higher participation rate from the public, but then you will also have cases such as Person X who may be spreading the illness under the impression that they are not infected due to a false-negative result.\nHow can we think about this carefully? This post is not medical instruction, but we can use data science tooling to help the average person better understand the complexities that public health officials face.\nDefinitions and grounding in reference Note In order to keep the mathematical discussion focused, we will have the following simplified assumptions:\nWe assume for simplicity that only one over-the-counter test is available to the public in order to keep our calculations simpler, and assume all batches are equally accurate and that it doesn\u0026rsquo;t ever expire. We also assume that the test accuracy remain unchanged for new strains, which is also not likely. We assume for simplicity the uniformity in PCR testing (the more accurate but slower test for COVID-19), that is, we assume every single test taken anywhere using any lab will have the same accuracy, and that no lab mistakes are made. We will ignore the variation in human biology and assume that every single person conducts the at-home-test in exactly the same perfectly accurate way. For example, we ignore the fact that some individuals will not swab well enough, and that some individuals may be more likely to give false positives due to genetic variation in how the disease may manifest, i.e. it may be possible that some individuals are more likely to get a predominantly lower respiratory tract infection with this disease, and some individuals may be more likely to get an upper-respiratory tract infection (in which case the rapid test may be more accurate). In reality, none of these assumptions hold true, and medical and public health officials have to consider all of these complexities and many more.\nAs will always be the case in Data Science, we are considering a model of our world that aligns in some ways with ours, but is far simpler by necessity.\nTip The art of Data Science is finding a key point at which we\u0026rsquo;ve added enough complexity to our model for it to provide actionable findings, but not so much complexity that our models are impractical to deploy and use. However, we must always remember that these findings are only guaranteed to apply accurately to the world of our model, and thus we need to be careful in applying them without caution and testing to the world we live in. With those caveats in mind, let\u0026rsquo;s see what probability can tell us about our imagined Person X scenario. There are three main tests to determine if someone is positive for COVID-19, all of these tests have some tradeoffs:\nTest Type Pros Cons Rapid Test - Quick results, often within 15-30 minutes.\n- Non-invasive testing method. - Lower accuracy compared to PCR, higher chance of false negatives.\n- Less effective in detecting the virus early in infection. PCR - High accuracy and sensitivity.\n- Considered the gold standard for detecting active infections. - Results take longer, typically 1-3 days.\n- More invasive and may require specialized facilities. Antibody - Can detect past infections and immune response.\n- Useful for seroprevalence studies. - Not very useful in detecting active infections.\n- Takes 1-3 weeks after infection to develop detectable levels of antibodies. For all three types of tests, we can find multiple examples in the literature which have measured the accuracy of these tests, but of course, all of these studies are snapshots in time. As new strains become prevalent, the accuracy rate of, say, the Rapid Test, may drift. Additionally, given that research and data is never crystal clear, some of these studies disagree. For example, a study published in BMJ in 2022 found a substantially higher false positive rates with rapid testing than prior studies (Mulchandani et al)1 had found. Data is rarely clean, there is rarely a solid answer to questions we have to ask about our models, and so we sometimes have to make careful judgment calls on what data we should include or not.\nWarning We are using a single study here to ground our discussion in numbers. In no way should this single study be seen as the final source of truth, nor should the reader take numbers presented from this study as accurately applying to all tests. The reader is encouraged to conduct further research if they wish to be a part of any debates on the accuracy of these tests and how to present accuracy data to the public. The results summary in this journal article are informative to our discussion. We can very clearly see here why these fine probabilistic details aren\u0026rsquo;t part of public-health\u0026rsquo;s general public messaging. We will include as an exercise at the end of this chapter a request that the reader attempt to fashion a public-health message under the assumption that these findings are correct and replicated, which could in fact include a choice to not mention them at all. Their results summary reads (Mulchandani et al)1:\nTest result bands were often weak, with positive/negative discordance by three trained laboratory staff for 3.9% of devices. Using consensus readings, for known positive and negative samples sensitivity was 92.5% (95% confidence interval 88.8% to 95.1%) and specificity was 97.9% (97.2% to 98.4%). Using an immunoassay reference standard, sensitivity was 94.2% (90.7% to 96.5%) among PCR confirmed cases but 84.7% (80.6% to 88.1%) among other people with antibodies. This is consistent with AbC-19 being more sensitive when antibody concentrations are higher, as people with PCR confirmation tended to have more severe disease whereas only 62% (218/354) of seropositive participants had had symptoms. If 1 million key workers were tested with AbC-19 and 10% had actually been previously infected, 84,700 true positive and 18,900 false positive results would be projected. The probability that a positive result was correct would be 81.7% (76.8% to 85.8%).\nIn the next section, we\u0026rsquo;ll take a deeper dive into what this results summary may mean.\nDefinitions Test results First, let\u0026rsquo;s define some of the words used here that may be unfamiliar to those starting off in data science.\n1 2 3 4 5 6 7 flowchart TD A[Test Result] --\u0026gt;|+| B[Positive] A --\u0026gt;|- | C[Negative] B --\u0026gt;|Reality: Infected| D[True Positive] B --\u0026gt;|Reality: Not Infected| E[False Positive] C --\u0026gt;|Reality: Not Infected| F[True Negative] C --\u0026gt;|Reality: Infected| G[False Negative] True/False Positive/Negative We will define these in the context of our current discussion about epidemiology. Suppose a test can have two results: positive or negative. Suppose that these results map with some accuracy to a corresponding state, such as infected or not-infected. Then we can define the following terms:\nTrue Positive (TP): When a test result is positive, and the reality is that the person is indeed infected.\nFalse Positive (FP): When a test result is positive, and the reality is that the person is NOT infected.\nTrue Negative (TN): When a test result is negative, and the reality is that the person is NOT infected.\nFalse Negative (FN): When a test result is negative, and the reality is that the person is indeed infected.\nConditional Probability Suppose we wish to find out the probability that someone is actually infected when they have tested negative.\nConditional probability is defined as:\n$$ A \\cap B = \\{x \\in \\mathcal{U} \\mid x \\in A \\text{ and } x \\in B\\}$$ $$ P(B|A) = \\frac{P(A \\cap B)}{P(A)}$$ Consider a sample of two people whom we can definitively divide into a group whose test result was negative (group A) and those who were actually positive (group B). The group A might be those whose rapid test claimed negative, the group B might be those whose antibody test later showed they were actually infected.\n\\(A \\cap B\\) is the intersection of group A with group B, that is, people who belong to both groups\u0026ndash;people whose rapid test said negative but were actually positive, i.e. the False Negative group.\n\\(P(B | A)\\) is the mathematical way of stating the probability of being in group B (infected) given that one is also in group A (tests negative).\nWe define \\(P(B | A)\\) as the ratio \\(P(B \\cap A)\\) over \\(P(A)\\)\nHence, intuitively, the probability that of any randomly selected person in group A (test negative) are also in the pool B (is infected) is equal to the portion of people \\(A \\cap B\\) that overlaps with the entire pool A.\nThis assumes a very large ensemble of people selected to study in either group, this doesn\u0026rsquo;t work reliable for smaller sample sizes.\nSensitivity, Specificity, and Accuracy We now present some standard definitions:\nSensitivity Portion of test results which gave a true positive result out of the pool of all test subjects who were in fact infected (those who were infected and got an accurate positive result are TP, those who were infected but got a negative result were FN, the sum TP + FN gives us the total that was in fact infected). The probability is written as the probability of getting a positive result \\(P(+)\\) given \u0026ldquo;|\u0026rdquo; that they are indeed infected \\(P(D)\\):\n$$ P(+|D) = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $$\nSpecificity Portion of test results which gave a true negative result out of the pool of all test subjects who were in fact NOT infected (those who were NOT infected and got a positive result are FP, those who were NOT infected and got an accurate negative result were TN, the sum TN + FP gives us the total that was in fact NOT infected). The probability is written as the probability of getting an negative result \\(P(-)\\) given \u0026ldquo;|\u0026rdquo; that they are not infected \\(P(\\neg D)\\):\n$$ P(-|\\neg D) = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} $$\nAccuracy Portion of all test results that were correctly identified, either as positive or negative, out of the total number of tests conducted. This includes both true positive results (TP, those who were infected and got an accurate positive result) and true negative results (TN, those who were NOT infected and got an accurate negative result), divided by the total number of tests, which is the sum of true positives (TP), false positives (FP, those who were NOT infected but got a positive result), true negatives (TN), and false negatives (FN, those who were infected but got a negative result). $$ \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} $$ Remark In what follows, we will be treating this equation and other like it as if this equivalence were true:\n$$ \\text{Specificity} = P(\\text{TN}|\\neg D) = P(-|\\neg D) $$\nIn the context of disease as we are discussing it here, this equivalence is valid. Please note, though, that for more complicated discussions on probability, this equivalence may not hold.\nAnalysis of journal article Given the above definitions, we can examine this statement of the results from our example reference (Mulchandani et al)1:\nUsing consensus readings, for known positive and negative samples sensitivity was 92.5% (95% confidence interval 88.8% to 95.1%) and specificity was 97.9% (97.2% to 98.4%).\nBy consensus readings, the protocol they describe entailed having three readers evaluate each test (Mulchandani et al)1:\nIf the three independent readers disagreed on the positivity of a sample, the majority reading was taken as the “overall” or consensus test result in our primary analysis, as per the WHO protocol.\nTo determine who was true positive, the researchers used the baseline of truth for this consensus reading analysis to be \u0026ldquo;participants who had had a positive PCR test for SARS-CoV-2\u0026rsquo;\u0026rsquo; (Mulchandani et al)1. The PCR test, while more accurate than a rapid test, itself lacks perfect accuracy, so this and other systematic potential errors results in a uncertainty interval. Here\u0026rsquo;s a quick summary of their sensitivity findings:\nSummary of findings Rapid Test Sensitivity when baseline for true positives was self-reported PCR positives alone: ≈92.5% Rapid Test Sensitivity for those who had confirmed PCR positives when baseline for true positives was antibody blood test: ≈94.2% Rapid Test Sensitivity for those who had no PCR test and hence had an unknown infection status, when baseline for true positives was antibody blood test: ≈84.7% So what\u0026rsquo;s happening here? One explanation the researchers propose is that those individuals who had a more intense infection were more likely to experience more intense symptoms and more likely to get a PCR test. And since Rapid Tests are shown to be more accurate for those with more intense symptoms, and having more intense symptoms makes getting a PCR more likely (due to the inconvenience and discomfort of these tests as compared to Rapid Tests, those with fewer or less intense symptoms are less likely to make the effort), it then makes sense that these tests have higher sensitivity for those who got the PCR test. In the words of authors (Mulchandani et al)1:\nThis is consistent with AbC-19 being more sensitive when antibody concentrations are higher, as people with PCR confirmation tended to have more severe disease whereas only 62% (218/354) of seropositive participants had had symptoms.\nThat is, of their sample group with whom they were certain had been positive due to the seropositive (anti-body blood test), only 62% had displayed symptoms. That leaves a large group of people who might have shown no symptoms and did not know they were exposed, and those who had no symptoms, knew they were exposed, but either didn\u0026rsquo;t bother getting tested due to lack of symptoms or simply took a rapid test.\nSynthesis Let\u0026rsquo;s imagine for a moment the unenviable task of being in charge of public health messaging. How might you relate the prior discussion to the general public? (Please note, in reality you would want to synthesize many journal articles that replicate each other, but when there is a new virus on the scene, often one has to work with just the first to publish and decide whether or not to act on that information before replication is achieved by other labs).\nWe can look at how some of this information was handled at the time. One example dated from August 2020 (well before the above journal article was published) was that the US FDA advised at the time that rapid test should not be given to people without symptoms (Mulchandani et al)1:\nPeople without symptoms of COVID-19 who haven’t been exposed to the virus shouldn’t get rapid tests to see if they are infected, according to guidance Friday from the Food and Drug Administration.\nThe guidance, added to the agency’s website, says that instead, highly sensitive tests, known as PCR tests, should be used for such individuals — if turnaround times are fast enough. These lab-based tests are known to be more accurate, but take hours to complete. Recent backlogs across the country have left some people waiting upward of 10 days for results.\nAt the time, rapid tests were approximately the same cost as PCR tests. From the University of Chicago News we can find a more recent recommendation from Assoc. Prof. Emily Landon \\cite{UChicagoNews2020RapidCOVIDTest}:\nRapid antigen tests – which you can buy in most pharmacies, big box stores and online retailers, are an excellent choice – but you may need to take multiple tests. Rapid antigen tests detect COVID-19 when people have a higher amount of virus particles in their system and are more contagious. But a negative antigen test doesn’t necessarily mean you don’t have COVID-19. Trust a positive antigen test, but be more skeptical about a negative one.\nDiscussion of journal article We agree with the journal authors that it is advisable to \u0026ldquo;trust a positive [rapid test]\u0026rdquo;, even though the sensitivity has been found to be potentially as low as 84.7% for the test in question at the time of testing. We can argue from a public good perspective that this is an excellent example of public health messaging:\nFirstly, we have to accept that the scientific education of the general public is on a spectrum, and that the vast majority of the public lacks scientific education beyond what was provided in public schooling. This is not to sound elitist, but we have to understand our audience and what an audience may hear from their perspective. If enough of the public starts to think that because of their imperfections, the rapid tests aren\u0026rsquo;t worth taking, then a powerful tool for slowing the spread of infection is reduced. For those who might have a non-Covid infection, such as a cold, and test positive with the rapid test, the likely worst outcome of the false positive is that they take extra precautions to not spread their infection. On the other hand, if they were actually positive and dismiss the test\u0026rsquo;s positive results, they could potentially infect individuals who are vulnerable to severe disease, or at least be more likely to spread the disease than otherwise. So instead of saying to the public: \u0026ldquo;If you test positive with a rapid test, you have an approximately 90% chance or so that you actually have COVID-19,\u0026rdquo; a simple straight-forward message to \u0026ldquo;trust the positive result\u0026rdquo; likely maximizes the public good.\nOn the other hand, although the specificity (portion of negatives that are true negatives) for these tests is fairly high, this is partially due to the fact that in the pool of test subjects, the vast majority of the subjects will not have COVID-19, and since the total number of False Positives will be small portion of the already relatively small number of positives, True Negatives will dominate the specificity equation; or to think about it in terms of limits:\n$$ \\lim_{TN \\rightarrow \\infty} \\frac{TN}{TN + FP} = 1 $$\nThings bring us to an important remark:\nRemark When evaluating the values of sensitivity and specificity, be mindful of the proportions of the pool being analyzed. For example, if a test is administered to a population of 1000 where only one single person is infected, and the test captures that positive and also gives a single false positive, then specificity becomes: $$ P(-|\\neg D) = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} = \\frac{999}{998 + 1} = 1$$ That is, everyone who was negative was measured to by the test, but the sample size is so small that this measure is unlikely to be a reliable projection for a larger population.\nSimilarly, sensitivity becomes: $$ P(+|D) = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} = \\frac{1}{1 + 0 } = 1 $$ However, these numbers are practically worthless because the sample set was so heavily unbalanced and there isn\u0026rsquo;t enough subjects to form a clear statistical picture.\nNeither of these measures reflect that we had a single false positive, although accuracy would:\n$$ \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}} = \\frac{1 + 998}{1 + 998 + 1 + 0} = 0.999 $$\nBut again, given the small sample size, these numbers are not likely to hold up upon replication with a larger sample size. Statistics is about forming conclusions based on incomplete data, but there is a limit at which data is too incomplete to draw conclusions. How that limit is determined is beyond the scope of this post.\nHowever, in the study cited, and a multitude of other studies, the results are statistically significant enough that we can, with some confidence, examine real world scenarios with the context of these findings. Let\u0026rsquo;s return to our case of Person X and examine their situation given these numbers and assumptions above.\nExample Example Suppose the Person X we started this post with, who has symptoms consistent with cold/flu/COVID-19, took a rapid test and got a negative result. What are the chances that Person X got a false negative and is indeed infected with SARS-COV-2?\nIn the following discussion, D is short for disease, such that \\( P(D) \\) is the probability that someone who exhibits certain symptoms has the disease in question, in this case, COVID-19, vs having some other disease \\(P(D\u0026rsquo;)\\), say, a flu.\nGiven the following assumptions and definitions:\nPrevalence Rate of COVID-19 in the population of those currently exhibiting cold and flu symptoms (Prior Probability, P(D)): 35%. This means that of all the people displaying cold and flu symptoms, 35% will have COVID-19, the remainder will have some other virus. I\u0026rsquo;ve randomly selected the number 35% for the sake of this example, in reality this number will vary by region and current viral trends. It is also impossible to determine this number precisely since a number of individuals who are showing such symptoms do not get sick enough to enter the medical system for testing, and for many viruses people might get effected and have minimal or no symptoms (asymptomatic) and be missed entirely. Sensitivity (True Positive Rate, P(+|D), the probability of getting a positive result given that the person does have the disease): 84.7%. Specificity (True Negative Rate, P(-|¬D), the probability of getting a negative result given that the individual does not have the disease): 97.9%. \\(P(-|D)\\): The probability of getting a false negative result given that the individual does have the disease (1-sensitivity). \\(P(D|-)\\): The probability of the tested individual to have the disease given a negative test. Let\u0026rsquo;s start by calculating \\(P(-)\\), the probability of any member of the subject population receiving a negative test result from the rapid test (without knowing whether they have the disease or not). There are two probabilistic pathways for them to fall into this group. The first is that they don\u0026rsquo;t have the disease and get a True Negative result from the test. The other pathway is that they have the disease but get a False Negative.\n1 2 3 4 5 6 7 graph TD Population --\u0026gt; |\u0026#34;P(D)\u0026#34;| D[\u0026#34;Has Disease (D)\u0026#34;] Population --\u0026gt; |\u0026#34;P(¬D)\u0026#34;| ND[\u0026#34;No Disease (¬D)\u0026#34;] D --\u0026gt; |\u0026#34;P(-|D): 1-Sensitivity\u0026#34;| FN[\u0026#34;False Negative\u0026#34;] ND --\u0026gt; |\u0026#34;P(-|¬D): Specificity\u0026#34;| TN[\u0026#34;True Negative\u0026#34;] FN --\u0026gt; |\u0026#34;P(D) · P(-|D)\u0026#34;| N[\u0026#34;Negative Result P(-)\u0026#34;] TN --\u0026gt; |\u0026#34;P(¬D) · P(-|¬D)\u0026#34;| N The probability of getting a negative result is thus the sum of the probability of having the disease but getting a false negative given that one has the disease, \\(P(D) \\cdot P(-|D)\\), plus the probability of not having the disease and getting a true negative result given that one does not have the disease, \\(P(\\neg D) · P(-|\\neg D)\\).\nThis gives us the equation\n1 2 3 4 5 6 \\begin{aligned} P(-) \u0026amp;=\u0026amp; P(\\neg D) \\cdot P(-|\\neg D) + P(D) \\cdot P(-|D) \\\\ \u0026amp;=\u0026amp; \\left(1 - P(D)\\right) \\cdot P(-|\\neg D) + P(D) \\cdot \\left(1 - P(+|D)\\right)\\\\ \u0026amp;=\u0026amp; (1-0.35)(0.979) + (0.35)(1-0.847) \\\\ \u0026amp;=\u0026amp; 0.6899 \\end{aligned} So for our Person X, we would like to know: given a negative test, what is the probability that they actually do have COVID-19? That is, we would like to estimate \\(P(D|-)\\). By the definition of conditional probabilities:\n1 2 3 4 5 6 7 \\begin{aligned} P(D|-) \u0026amp;=\u0026amp; \\frac{P(D \\cap -)}{P(-)} \\\\ \u0026amp;=\u0026amp; \\frac{P(D)\\cdot P(-|D)}{P(-)} \\\\ \u0026amp;=\u0026amp; \\frac{ P(D) \\cdot \\left( 1 - P(+|D)\\right) }{P(-)} \\\\ \u0026amp;=\u0026amp; \\frac{0.35 \\cdot (1-0.847)}{0.6899}\\\\ \u0026amp;=\u0026amp; 0.0776 \\end{aligned} That is, in this given scenario, assuming the numbers above, the probability that Person X has the disease despite the fact that their rapid test came back negative is around 7.76%. Important: This number is particular to the scenario we imagined above, but is in no way representative for every case. For example, we picked a random percentage for \\(P(D)\\). Please do not use these numbers to make real world decisions or arguments. This was for demonstrative educational purposes only.\nBayes\u0026rsquo; Theorem The above equation second line in the above question is known as Bayes\u0026rsquo; Theorem, which is generically written:\n$$P(A | B)=\\frac{P(A) \\cdot P(B | A) }{P(B)}$$\nDiscussion We can visualize the situation with a Sankey diagram. Here we exaggerate the percentage of false positives and negatives for visual purposes. As can be seen, those who are infected but obtain a negative test and then do not self-isolate join the uninfected in non-isolation allowing the virus to spread and underlining the importance of accurate tests. However, the test results are accurate enough to catch the majority of people who are genuinely infected, and hence, reduces the rate of spread.\nThis Bayesian analysis demonstrates that, under the given assumptions, an individual with symptoms who tests negative with a rapid COVID-19 test sometimes has a low but non-trivial probability of being actually positive for the virus.\nHowever, diseases aren\u0026rsquo;t just the story of an individual, they are fully stories of an ensemble of individuals.\nSo instead of saying that in this imagined scenario, Person X has a 7.76% chance of actually having COVID-19 despite the negative rapid test, we would more accurately have summarized it as follows:\nGiven these (guessed) assumptions, including the test accuracy rates and prevalence of COVID-19 among those exhibiting flu or cold like symptoms, on average 7 to 8 out of every 100 people chosen from the wider population who have symptoms of a respiratory illness but test negative with this rapid test will in fact be positive and potentially capable of spreading the disease. (Again, please note that these numbers are not to be used in the real world, this is meant to demonstrate a first pass at calculating these probabilities.) Hence we see the public health messaging challenges for scenarios such as this, where we need to underline the imperfections of the test in order to instill some caution in the public regarding negative tests results, while maintaining public trust in the positive results so that the population continues to take them seriously and self-isolate.\nUltimately, the work of public health comes down to minimizing the total number of deaths balanced against the negative impacts of public health mitigation efforts. It is a tough job, but one that benefits from public communications that are straight-forward to understand by the general population.\nUnderstanding how these probabilities are calculated can help individuals spot misinformation.\nIt is a tough job, with no easy answers, and as is typical in statistics, there is never any perfect certainty. Accepting that this is true for almost every decision that must be made in human societies doesn\u0026rsquo;t mean we just give up and let what happens happen. It means we do our very best with the limited information we have, and constantly refine our approach as more information is obtained.\nMulchandani et al., Accuracy of UK Rapid Test Consortium (UK-RTC) AbC-19 Rapid Test for detection of previous SARS-CoV-2 infection in key workers: test accuracy study, BMJ, 371, 2020, doi 10.1136/bmj.m4262\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-08-02T15:21:05-07:00","permalink":"https://tdj28.github.io/p/bayesian-look-at-rapid-tests/","title":"Bayesian look at rapid tests"},{"content":"Introduction META\u0026rsquo;s recent release of Segment Anything 2 (SAM2) as a fully open-source project, where both the code and the models are open-source, opens the door for many interesting use cases. One use case is people detection and tracking.\nSuppose, for example, that I wanted to detect and track if people enter some region in my camera\u0026rsquo;s view. The YOLO toolset has been widely used for people detection, and here we use it to detect people whose YOLO-created bounding box intersects with a chosen detection region. Then we use points around the center of that bounding box as seeds for tracking those individuals with SAM2.\nWarning\nThis code is shared purely for research purposes and should not be used as-is for anything beyond education. The reliability of these tools in production settings needs to be tested thoroughly\nCode Code for this post can be found here.\nProcedure Create synthetic camera footage with Runway ML Gen 3 Pick a frame early in the synthetic footage, in this case, frame 30 Designate a detection region in the image Get people detection bounding boxes from the image Get a subset of detection bounding boxes which intersect with the detection region Get the center point of the detection bounding boxes in that subset, and use those as seeds for SAM2 to track those individuals through the remainder of the video Setup First, we need to set up the environment and install the required packages:\nTip\nThe pinning of package versions here is to help ensure that this code can work as is out-of-the-box; however, you may get better results by unpinning these versions and just using the latest code. It is generally best practice to pin packages in production environments.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Install necessary packages !pip install ultralytics==8.2.71 !pip install torch==2.4.0 !pip install torchvision==0.19.0 !pip install torchaudio==2.4.0 !pip install matplotlib==3.9.1 !pip install pillow !pip install opencv-python-headless==4.10.0.84 # Clone the Segment Anything 2 repository and install it !git clone https://github.com/facebookresearch/segment-anything-2.git %cd segment-anything-2 !pip install -e . -q # Download the pre-trained SAM2 model !wget -q https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt -P checkpoints Loading and Processing the Video We start by converting the input video into frames. The frames will be processed to detect and segment people.\nInfo\nAs of the publication of this post, SAM2 only supports JPEG.\nFirst we do some initial setup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import os import matplotlib.pyplot as plt from PIL import Image import matplotlib.patches as patches import torch import numpy as np import cv2 from ultralytics import YOLO from sam2.utils.misc import get_sdpa_settings from sam2.build_sam import build_sam2_video_predictor from sam2.build_sam import build_sam2 from sam2.sam2_image_predictor import SAM2ImagePredictor OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings() print(f\u0026#34;{OLD_GPU}, {USE_FLASH_ATTN}, {MATH_KERNEL_ON}\u0026#34;) torch.autocast(device_type=\u0026#34;cuda\u0026#34;, dtype=torch.bfloat16).__enter__() if torch.cuda.get_device_properties(0).major \u0026gt;= 8: torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.allow_tf32 = True DEVICE = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) CHECKPOINT = f\u0026#34;{HOME}/checkpoints/sam2_hiera_large.pt\u0026#34; CONFIG = \u0026#34;sam2_hiera_l.yaml\u0026#34; #sam2_model = build_sam2(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False) predictor = build_sam2_video_predictor(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False) Next we extract jpg frames from the mp4 video we uploaded to the videos folder:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # Define directories video_dir = f\u0026#34;{HOME}/videos/\u0026#34; processed_dir = f\u0026#34;{HOME}/processed\u0026#34; # Create the processed directory if it doesn\u0026#39;t exist os.makedirs(video_dir, exist_ok=True) os.makedirs(processed_dir, exist_ok=True) def convert_mp4_to_jpg(mp4_file, output_folder, one_frame_per_second=False): \u0026#34;\u0026#34;\u0026#34;Converts an mp4 file to high-quality jpg files.\u0026#34;\u0026#34;\u0026#34; os.makedirs(output_folder, exist_ok=True) video = cv2.VideoCapture(mp4_file) fps = video.get(cv2.CAP_PROP_FPS) frame_count = 0 saved_frame_count = 0 while True: ret, frame = video.read() if not ret: break if one_frame_per_second: # Calculate the frame number to save if frame_count % int(fps) == 0: output_file = os.path.join(output_folder, f\u0026#34;{saved_frame_count:04d}.jpg\u0026#34;) success = cv2.imwrite(output_file, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95]) if success: pass #print(f\u0026#34;Frame {saved_frame_count} written successfully\u0026#34;) else: print(f\u0026#34;Failed to write frame {saved_frame_count}\u0026#34;) saved_frame_count += 1 else: output_file = os.path.join(output_folder, f\u0026#34;{frame_count:04d}.jpg\u0026#34;) success = cv2.imwrite(output_file, frame, [int(cv2.IMWRITE_JPEG_QUALITY), 95]) if success: pass #print(f\u0026#34;Frame {frame_count} written successfully\u0026#34;) else: print(f\u0026#34;Failed to write frame {frame_count}\u0026#34;) frame_count += 1 video.release() mp4_file_path = f\u0026#34;{HOME}/videos/input_video.mp4\u0026#34; # Replace with your mp4 file path output_folder_path = f\u0026#34;{HOME}/processed\u0026#34; convert_mp4_to_jpg(mp4_file_path, output_folder_path, one_frame_per_second=False) Finally, we put all the frame file paths in a list for later processing:\n1 2 3 4 5 6 7 8 # Scan all the frame names in the processed directory processed_frame_names = [ p for p in os.listdir(output_folder_path ) if os.path.splitext(p)[-1].lower() in [\u0026#34;.jpg\u0026#34;, \u0026#34;.jpeg\u0026#34;] ] # Sort frame names by extracting the numerical part of the filenames processed_frame_names.sort(key=lambda p: int(os.path.splitext(p)[0].split(\u0026#39;_\u0026#39;)[-1])) People Detection and Segmentation Detecting people with YOLO Using YOLO, we detect people in the frames and highlight those whose bounding boxes fall within a designated red zone.\nWe pick which frame to detect in and set an image_path variable:\n1 2 3 4 # which frame to detect on and set an image_path variable frame_idx = 30 image_path = os.path.join(output_folder_path, processed_frame_names[frame_idx]) image = cv2.imread(image_path) Next we add a function to plot the detection region:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Define and add the rectangle patch with transparency rect_x = 380 rect_y = 380 rect_width = 330 rect_height = 200 # Define a function to plot the figure and rectangle patch def plot_with_rect(ax, image, rect_x, rect_y, rect_width, rect_height): # Display the original image ax.imshow(image) # Create and add the rectangle patch rect = patches.Rectangle( (rect_x, rect_y), rect_width, rect_height, linewidth=1, edgecolor=\u0026#39;r\u0026#39;, facecolor=\u0026#39;red\u0026#39;, alpha=0.3 ) ax.add_patch(rect) Now we perform the people detections:\nWarning\nWe are using YOLOv5 here, but there are newer version available.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 image = Image.open(image_path) # Display the image plt.figure(figsize=(12, 8)) ax = plt.gca() plt.title(f\u0026#34;frame {frame_idx}\u0026#34;) plt.imshow(image) # Add the patch to the Axes plot_with_rect(ax, image, rect_x, rect_y, rect_width, rect_height) # Load YOLO model yolo_model = YOLO(\u0026#39;yolov5su.pt\u0026#39;) # Use the appropriate YOLOv5 model # Load the image with OpenCV cv2_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR) # Detect objects in the image results = yolo_model(cv2_image) # Filter detections to find people detected_people_centers = [] filtered_boxes = [] for result in results[0].boxes.data: # Accessing the first result and its boxes x1, y1, x2, y2, conf, cls = result.cpu().numpy() # Transfer to CPU and convert to numpy array if int(cls) == 0: # Class 0 is \u0026#39;person\u0026#39; in COCO if (x1 \u0026lt; rect_x + rect_width and x2 \u0026gt; rect_x and y1 \u0026lt; rect_y + rect_height and y2 \u0026gt; rect_y): center_x = (x1 + x2) / 2 center_y = (y1 + y2) / 2 detected_people_centers.append([center_x, center_y]) filtered_boxes.append((x1, y1, x2, y2)) # Draw bounding box plt.gca().add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor=\u0026#39;b\u0026#39;, facecolor=\u0026#39;none\u0026#39;)) # Convert detected people centers to a numpy array points = np.array(detected_people_centers) labels = np.ones(len(points), dtype=np.int32) # Display the updated plot with bounding boxes plt.show() Initializing the Segment Anything 2 Predictor We initialize the SAM2 predictor to generate masks for each detected person as follows.\nNote\nYou\u0026rsquo;ll notice that in addition to using the point of center of the bounding boxes, we use two more points slightly above it. This is done to improve the SAM2 detection results. The YOLO bounding box center is typically around the midsection of a detected individual, so selecting a few points above helps ensure we target the whole individual more reliably with SAM2.\nWe use some functions to help us with plotting and DRYing up some detections. These first two functions come directly from SAM2\u0026rsquo;s example notebook:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def show_mask(mask, ax, obj_id=None, random_color=False): if random_color: color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) else: cmap = plt.get_cmap(\u0026#34;tab10\u0026#34;) cmap_idx = 0 if obj_id is None else obj_id color = np.array([*cmap(cmap_idx)[:3], 0.6]) h, w = mask.shape[-2:] mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1) ax.imshow(mask_image) def show_points(coords, labels, ax, marker_size=200): pos_points = coords[labels==1] neg_points = coords[labels==0] ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\u0026#39;green\u0026#39;, marker=\u0026#39;*\u0026#39;, s=marker_size, edgecolor=\u0026#39;white\u0026#39;, linewidth=1.25) ax.scatter(neg_points[:, 0], neg_points[:, 1], color=\u0026#39;red\u0026#39;, marker=\u0026#39;*\u0026#39;, s=marker_size, edgecolor=\u0026#39;white\u0026#39;, linewidth=1.25) We also have a few helper functions to make our code DRY/cleaner:\n1 2 3 4 5 6 7 8 9 10 def find_center_point(x1, y1, x2, y2): cx = (x1 + x2) / 2 cy = (y1 + y2) / 2 return [(cx, cy)] def add_points_around_bbox(x1, y1, x2, y2, distance): cx = (x1 + x2) / 2 cy = (y1 + y2) / 2 return [(cx, cy), (cx, cy + distance), (cx, cy - 3*distance)] This is the important loop that applies SAM2 to the detected individuals:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 from sam2.utils.misc import get_sdpa_settings from sam2.build_sam import build_sam2_video_predictor OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings() DEVICE = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) CHECKPOINT = f\u0026#34;{HOME}/checkpoints/sam2_hiera_large.pt\u0026#34; CONFIG = \u0026#34;sam2_hiera_l.yaml\u0026#34; predictor = build_sam2_video_predictor(CONFIG, CHECKPOINT, device=DEVICE, apply_postprocessing=False) # Load and display the initial frame with masks def show_mask(mask, ax, obj_id=None, random_color=False): if random_color: color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0) else: cmap = plt.get_cmap(\u0026#34;tab10\u0026#34;) cmap_idx = 0 if obj_id is None else obj_id color = np.array([*cmap(cmap_idx)[:3], 0.6]) h, w = mask.shape[-2:] mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1) ax.imshow(mask_image) def show_points(coords, labels, ax, marker_size=200): pos_points = coords[labels==1] neg_points = coords[labels==0] ax.scatter(pos_points[:, 0], pos_points[:, 1], color=\u0026#39;green\u0026#39;, marker=\u0026#39;*\u0026#39;, s=marker_size, edgecolor=\u0026#39;white\u0026#39;, linewidth=1.25) ax.scatter(neg_points[:, 0], pos_points[:, 1], color=\u0026#39;red\u0026#39;, marker=\u0026#39;*\u0026#39;, s=marker_size, edgecolor=\u0026#39;white\u0026#39;, linewidth=1.25) with torch.inference_mode(), torch.autocast(\u0026#34;cuda\u0026#34;, dtype=torch.bfloat16): inference_state = predictor.init_state(video_path=output_folder_path) plt.figure(figsize=(12, 8)) ax = plt.gca() # Display the original image ax.imshow(image) # Add the patch to the Axes plot_with_rect(ax, image, rect_x, rect_y, rect_width, rect_height) # Track initial masks for each detected person initial_points = {} initial_masks = {} for ann_obj_id, (x1, y1, x2, y2) in enumerate(filtered_boxes, start=1): # Generate points within the bounding box and additional points above and below bbox_height = y2 - y1 additional_distance = 0.05 * bbox_height extended_points = add_points_around_bbox(x1, y1, x2, y2, additional_distance) extended_points = np.array(extended_points) extended_labels = np.ones(len(extended_points), dtype=np.int32) # Store the initial points for later use initial_points[ann_obj_id] = (extended_points, extended_labels) # Add new points for the first frame (using the extended points) if extended_points.size \u0026gt; 0: predictor.reset_state(inference_state) _, out_obj_ids, out_mask_logits = predictor.add_new_points( inference_state=inference_state, frame_idx=frame_idx, obj_id=ann_obj_id, points=extended_points, labels=extended_labels, ) initial_masks[ann_obj_id] = (out_mask_logits[0] \u0026gt; 0.0).cpu().numpy() # Draw the YOLO bounding box ax.add_patch(patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor=\u0026#39;b\u0026#39;, facecolor=\u0026#39;none\u0026#39;)) # Overlay the SAM2 mask show_mask(initial_masks[ann_obj_id], ax, obj_id=ann_obj_id) show_points(extended_points, extended_labels, ax) # Show the final plot with the highlighted people plt.show() Propagating SAM2 Detections Forward Throughout the Video We propagate the masks generated in the first frame through the entire video.\nNote\nThe way we are doing this here is not efficient as we are propagating one detected person at a time, and then applying the masks later to the final image. This was done because detecting multiple people at once was showing some unwanted artifacts. In a future iteration of this, we will try to do the propegation for all detected points at the same time rather than via loops.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # Initialize a dictionary to store masks per frame per person video_segments = {} # Run propagation for each detected person individually with torch.inference_mode(), torch.autocast(\u0026#34;cuda\u0026#34;, dtype=torch.bfloat16): for ann_obj_id, (points, labels) in initial_points.items(): # Reset the state for the current object inference_state = predictor.init_state(video_path=output_folder_path) predictor.reset_state(inference_state) predictor.add_new_points( inference_state=inference_state, frame_idx=frame_idx, obj_id=ann_obj_id, points=points, labels=labels, ) # Propagate the masks for the current object throughout the video for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state): if out_frame_idx not in video_segments: video_segments[out_frame_idx] = {} if ann_obj_id in out_obj_ids: video_segments[out_frame_idx][ann_obj_id] = (out_mask_logits[out_obj_ids.index(ann_obj_id)] \u0026gt; 0.0).cpu().numpy() We save the output frames as PNG for better quality (though at a cost of more disk space):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Define the output directory for the frames output_dir = os.path.join(f\u0026#34;{HOME}\u0026#34;, \u0026#39;output_frames\u0026#39;) os.makedirs(output_dir, exist_ok=True) # Render the segmentation results every x frame and save as PNG vis_frame_stride = 1 plt.close(\u0026#34;all\u0026#34;) frame_paths = [] for out_frame_idx in range(0, len(processed_frame_names), vis_frame_stride): plt.figure(figsize=(6, 4)) plt.title(f\u0026#34;frame {out_frame_idx}\u0026#34;) plt.imshow(Image.open(os.path.join(processed_dir, processed_frame_names[out_frame_idx]))) if out_frame_idx in video_segments: for out_obj_id, out_mask in video_segments[out_frame_idx].items(): show_mask(out_mask, plt.gca(), obj_id=out_obj_id) frame_path = os.path.join(output_dir, f\u0026#34;frame_{out_frame_idx:04d}.png\u0026#34;) plt.savefig(frame_path) frame_paths.append(frame_path) plt.close() Generating the Output Video Finally, we compile the processed frames into an output video.\n1 2 3 4 5 6 7 8 9 10 11 12 13 from moviepy.editor import VideoFileClip, ImageSequenceClip # Path to the input video input_video_path = os.path.join(f\u0026#34;{HOME}\u0026#34;, \u0026#39;videos\u0026#39;, \u0026#39;input_video.mp4\u0026#39;) # Detect the frame rate of the input video video_clip = VideoFileClip(input_video_path) frame_rate = video_clip.fps # Create a video from the saved frames using moviepy video_output_path = os.path.join(f\u0026#34;{HOME}\u0026#34;, \u0026#39;videos\u0026#39;, \u0026#39;output_video.mp4\u0026#39;) clip = ImageSequenceClip(frame_paths, fps=frame_rate) clip.write_videofile(video_output_path, codec=\u0026#39;libx264\u0026#39;) The resulting video demonstrates how SAM2 maintains distinct object tracking throughout the video duration:\nNote how the occlusion of the person masked in green by the person masked in orange does not cause the loss of tracking for the person masked in green. Even more impressive, the person masked in red is not lost by SAM2 despite being heavily occluded.\nConclusions We demonstrated a way to detect and track people seen passing through a designated detection region in a video using YOLO to bootstrap SAM2. Next steps include:\nCleaning up the code, de-looping the detection (see note in the SAM2 process section) Dockerizing this process Exploring monocular depth estimation as a way to more accurately detect proximity: Monocular depth estimation uses trained models that can pick up lighting cues to recreate the information related to the z axis in 2D video. In a subset of cases (perhaps even the majority of cases?), this recreation of z-axis depth perception is accurate enough to provide reliable detection of proximity in all three dimensions of space. Monocular depth estimation is being studied, for example, by Toyota in regards to autonomous vehicle driving. Further, the related field of Gaussian Splat is a very active area of research. ","date":"2024-08-01T19:21:05-07:00","permalink":"https://tdj28.github.io/p/using-yolo-people-detection-with-segment-anything-2-from-meta/","title":"Using YOLO people detection with Segment Anything 2 from META"},{"content":"\nDownload the corresponding notebook.\nIntroduction Kerbal Space Program (KSP) (1.0) is a semi-physics-realistic space exploration simulation. While it is not realistic enough to plan actual space missions, it is a fun educational tool that can get us started. With the kRPC mod (remote procedure call, which enables programatic interaction with KSP\u0026rsquo;s API), the educational potential is expanded into the world of programming. In this notebook, we will automate taking the Kerbins to the moon and back using Python, Physics, and KSP with the kRPC mod.\nInstallation Get KSP Purchase and install the Kerbal Space Program (v 1.0) here. You\u0026rsquo;ll also need the \u0026ldquo;Making History\u0026rdquo; expansion which has the Apollo rocket equivalent, unless you prefer to save some money and build an equivalent rocket yourself or find the ship file online. Note that this code is tested and verified to work with the Acopollo rocket found in this expansion, so you may have to make adjustments to get alternative rockets to work. Install the KRPC mod Follow this guide to install the kRPC mod. More or less, you\u0026rsquo;ll grab the kRPC folder and put it in the GameData folder wherever Kerbal is installed on your workstation.\nPrepping the jupyter workspace If you are using Windows, installing anaconda may be your quickest route, and then set up a kerbal environment by opensing a terminal and running:\n1 conda create --name kerbal You\u0026rsquo;ll only need to create the envrionment once. Next activate it:\n1 conda activate kerbal If you are on MacOS or Linux, you can use virtualenv instead (although conda can work too).\nand then install the following packages:\n1 2 3 4 5 pip install setuptools==57.5. pip install protobuf~=3.19.0 pip install krpc pip install jupyter pip install ipykernel Or preferably, use a requirements file:\n1 2 3 4 5 setuptools==57.5.0 protobuf~=3.19.0 krpc jupyter ipykernel If you are using Python \u0026gt;= 3.8, you\u0026rsquo;ll have to do a little hack. Open the file in your virtual environment files with an editor and make the following change.\nFILE: Should be in your virtual environment with something along the lines of venv/lib/python3.10/site-packages/krpc/types.py on line 222 and 227.\nReplace:\n1 collections.Iterable with\n1 collections.abc.Iterable Next, create the kernel that we\u0026rsquo;ll be using for this notebook:\n1 python -m ipykernel install --user --name kerbal --display-name \u0026#34;Python (kerbal)\u0026#34; and then to launch jupyter:\n1 jupyter notebook Going to the Mun Connecting to KSP In our first code blocks, we will be importing the libraries we need to get our astronauts to the Mun via Python. We will then connect to KSP vis the krpc plugin (assumes you have KSP up and running and the plugin working).\n1 2 3 4 import krpc import math import time from IPython.display import clear_output, display 1 conn = krpc.connect() Loading the Apollo rocket 1 conn.space_center.load(\u0026#34;apollo001\u0026#34;) 1 2 3 4 5 6 vessel = conn.space_center.active_vessel # Set up streams for telemetry ut = conn.add_stream(getattr, conn.space_center, \u0026#39;ut\u0026#39;) altitude = conn.add_stream(getattr, vessel.flight(), \u0026#39;mean_altitude\u0026#39;) apoapsis = conn.add_stream(getattr, vessel.orbit, \u0026#39;apoapsis_altitude\u0026#39;) stage_2_resources = vessel.resources_in_decouple_stage(stage=2, cumulative=False) 1 2 obt_frame = vessel.orbit.body.non_rotating_reference_frame mun_orbit = conn.space_center.bodies[\u0026#34;Mun\u0026#34;].position(obt_frame) Gravity Turn into Orbit In this next section, we are calculating the gravity turn angle. The sample provided by the KRPC samples uses a simple fraction to define the trun angle in degrees: \\[ \\alpha = 90(\\frac{a - s}{ e - s}) \\] where \\(a\\) is the current altitude, \\(s\\) is the altitude in which we want to start slowly turning the rocket horizontally, and \\(e\\) is the ending altitude where we want to have completed our turn upon reaching. This could be defined by a simple function\n1 2 3 4 5 6 7 8 9 10 def get_gravity_turn_angle(alt, start_alt, end_alt, turn_angle): new_turn_angle = 0 if alt \u0026gt; start_alt and alt \u0026lt; end_alt: frac = (alt - start_alt)/(end_alt - start_alt) new_turn_angle = frac * 90.0 if abs(new_turn_angle - turn_angle) \u0026gt; 0.5: turn_angle = new_turn_angle else: turn_angle = 0 return turn_angle In a future blog post, we will improve the gravity turn by invoking Physics to get a more accurate turn rate. For now, as a proof of concept and to focus on the circularization physics, we will proceed with this rough linear gravity turn. Next, we actually launch the rocket and execute the gravity turn so that we reach a target apoapsis:\n1 2 3 vessel.auto_pilot.target_pitch_and_heading(90, 90) vessel.auto_pilot.engage() vessel.control.throttle = 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 vessel.control.activate_next_stage() turn_angle = 0 start_alt = 250 end_alt = 45000 target_alt = 700000 state = \u0026#34;LiftOff\u0026#34; while turn_angle \u0026gt;= 0 and vessel.control.throttle \u0026gt; 0: clear_output(wait=True) turn_angle = get_gravity_turn_angle(altitude(), start_alt, end_alt, turn_angle) if turn_angle != 0 and apoapsis() \u0026lt; target_alt: vessel.auto_pilot.target_pitch_and_heading(90-turn_angle, 90) state = \u0026#34;GravityTurn\u0026#34; if apoapsis() \u0026gt; target_alt*0.90 and apoapsis() \u0026lt; target_alt: state = \u0026#34;ApoapsisRefinement\u0026#34; vessel.control.throttle = 0.25 while apoapsis() \u0026lt; target_alt: pass if apoapsis() \u0026gt;= target_alt: state = \u0026#34;GoalApoapsisAchieved\u0026#34; vessel.control.throttle = 0.00 As the rocket climbs higher, it will ease into an orientation perpendicular to the radius of the Earth:\nCircularizing the orbit For an idealized circular orbit above atmospheric effects, the gravitational pull of the Earth on the body provides the centripetal force, with circular orbital radius \\(r\\) and let\n\\[\\mu = GM\\]\nwhere \\(G\\) is the universal gravitational constant and \\(M\\) is the mass of the large body (here, the Earth)\nKRPC provides an easy way to access that value for any given celestial body:\n1 mu = vessel.orbit.body.gravitational_parameter Back to Physics, we now have two equations from the basics of Newtonian physics, namely:\nCetripital Force: \\[ F_c = \\frac{m_0 v^2}{r}\\] Gravitational Force: \\[ F_g = \\frac{m_0 GM}{r^2}\\] Since these must balance out for an object to be in stead orbit, we can solve to find the ideal velocity \\(v\\) for a given orbit at radius \\(r\\):\n\\[ F_g = m_0 a = \\frac{m_0 \\mu}{r^2} =\\frac{m_0 v^2}{r} \\]\n\\[\\implies v = \\sqrt{ \\frac{\\mu}{r} }\\]\nWe see here that this velocity is entirely independent of the mass of the body, hence a very heavy object and a very light object which orbit at the same radius must be traveling at the same speed, assuming of course they are out of the region where atmospheric drag plays an important role.\nFinding an equation for the circularization of an orbit That is a starting point, but we have to dive deeper to calculate the numbers needed to actually achieve an approximation of this idealized orbit.\nOur next step is to invoke the vis-viva equation (or also called the orbital-energy-invariance law). We begin by noting that the specific orbital energy of an orbiting object and the body it is orbiting is defined as a sum of their mutual potential energy and their total kinetic energy:\n\\[ E_O = E_p + E_k\\]\nFor scenarios such as ours, where the orbiting object is much smaller in both size and mass than the body it orbits, this energy becomes easier to calculate since we can treat the larger body as being the center of our reference frame and hence at rest. We can also neglect the potential energy experienced by the gravitational attraction of the larger body by the smaller body. In this case,\n\\[ E_O = \\frac{m_0 v^2_0}{2} - \\frac{\\mu m_0}{r_0} \\]\nThe specific total energy is given as \\(E_o/m_0 = \\epsilon\\) and this is conserved throughout the orbit (assuming the rocket is not burning fuel in an idealized orbit). Let \\(v_a\\) and \\(r_a\\) be the velocity and radius of the orbiting body at the apoapsis, and \\(v_p\\) and \\(r_p\\) similarly at the periapsis. Then energy conservation gives us the relation,\n\\[ \\epsilon = \\frac{v_a^2}{2} - \\frac{\\mu}{r_a} = \\frac{v_p^2}{2} - \\frac{\\mu}{r_p} \\]\nAt both \\(r_a\\) and \\(r_p\\), the velocity and radius vectors are perpendicular to each other, where hence we can invoke the conservation of angular momentum to give\n\\[ r_p v_p = r_a v_a = Constant \\]\nThis gives us the relation that \\[ v_p =- \\frac{r_a}{r_p}v_a \\]\nand with some rearrangement of these equations, we get,\n\\[ \\frac{1}{2}\\left( \\frac{r_p^2 -r_a^2}{r_p^2}\\right) v_a^2 = \\mu \\left(\\frac{1}{r_a} - \\frac{1}{r_p}\\right) \\] \\[ \\frac{1}{2}v_a^2 = \\mu \\frac{r_p}{r_a \\left( r_p + r_a \\right)} \\]\nRelating this to elipses, the apoapsis and periapsis From geometry of an ellipse that relates the semimajor axis to \\(r_p\\) and \\(r_a\\), \\(2a = r_p + r_a\\)\n1 2 r = vessel.orbit.apoapsis a1 = vessel.orbit.semi_major_axis We substitute 2a in the our prior kinetic energy equation: \\[ \\frac{1}{2}v_a^2 = \\mu \\frac{2a -r_a}{2 r_a a} \\] \\[ \\frac{1}{2}v_a^2 = \\mu \\left(\\frac{1}{r_a} - \\frac{1}{2a}\\right) \\]\n\\[ v_a^2 = \\mu \\left( \\frac{2}{r_a} - \\frac{1}{a}\\right) \\]\nIn a circularized orbit, \\(a = r\\), and so the velocity we need to have is\n\\[ v_a^2 = \\mu \\left( \\frac{2}{r_a} - \\frac{1}{r_a}\\right) = \\mu \\left( \\frac{1}{r_a}\\right) \\]\nThe \\(\\Delta v\\) we need to achieve is the difference between these two velocities,\n$$ \\Delta v = \\sqrt{ \\mu} \\left( \\sqrt{\\left( \\frac{1}{r_a}\\right) } - \\sqrt{\\left( \\frac{2}{r_a} - \\frac{1}{a}\\right)}\\right) $$ $$ = \\sqrt{\\frac{\\mu}{r_a}}\\left( 1 - \\sqrt{ \\frac{2a -r_a}{a}}\\right) $$\nIn krpc code,\n1 delta_v = math.sqrt(mu/r)*(1-math.sqrt((2*a1-r)/a1)) With this number in hand, we can do a little more math which will be discussed in more detail later to find the burn time. This calculation is highly dependent on the engine and design of the rocket in question:\n1 2 3 4 5 6 7 8 f = vessel.available_thrust Isp = vessel.specific_impulse * 9.82 m0 = vessel.mass m1 = m0 / math.exp(delta_v / Isp) fr = f / Isp burn_time = (m0 - m1) / fr 1 2 node = vessel.control.add_node( ut() + vessel.orbit.time_to_apoapsis, prograde=delta_v) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # https://krpc.github.io/krpc/tutorials/launch-into-orbit.html print(\u0026#39;Orientating ship for circularization burn\u0026#39;) vessel.auto_pilot.disengage() vessel.auto_pilot.sas = True time.sleep(.1) vessel.auto_pilot.sas_mode = vessel.auto_pilot.sas_mode.maneuver vessel.auto_pilot.wait() print(\u0026#39;Waiting until circularization burn\u0026#39;) burn_ut = ut() + vessel.orbit.time_to_apoapsis - (burn_time/2.) lead_time = 5 conn.space_center.warp_to(burn_ut - lead_time) # Execute burn print(\u0026#39;Ready to execute burn\u0026#39;) time_to_apoapsis = conn.add_stream(getattr, vessel.orbit, \u0026#39;time_to_apoapsis\u0026#39;) while time_to_apoapsis() - (burn_time/2.) \u0026gt; 0: pass vessel.auto_pilot.wait() print(\u0026#39;Executing burn\u0026#39;) vessel.control.throttle = 1.0 time.sleep(burn_time - 0.05) print(\u0026#39;Fine tuning\u0026#39;) vessel.control.throttle = 0.05 remaining_burn = conn.add_stream(node.remaining_burn_vector, node.reference_frame) #vessel.auto_pilot.engage() vessel.auto_pilot.disengage() vessel.auto_pilot.sas = True time.sleep(.1) vessel.auto_pilot.sas_mode = vessel.auto_pilot.sas_mode.maneuver vessel.control.throttle = 0.03 while node.remaining_delta_v \u0026gt; 4: pass vessel.control.throttle = 0.0 node.remove() time.sleep(5) print(\u0026#39;Launch complete\u0026#39;) Orientating ship for circularization burn Waiting until circularization burn Ready to execute burn Executing burn Fine tuning Launch complete If all went well, you should be in a fairly accurate (and we can make it even better!) circularized orbit:\nNow that we have circularized our orbit, we can do some house keeping and eject uneeded components from the rocket. In a future iteration, we will want to eject these as soon as we clear the atmosphere in order to reduce as much mass as soon as possible:\n1 2 3 4 5 6 # Drop stage 1 vessel.control.activate_next_stage() # Drop the escape rocket vessel.control.activate_next_stage() # Drop the shields to lower weight vessel.control.activate_next_stage() ","date":"2022-07-23T15:21:05-07:00","permalink":"https://tdj28.github.io/p/automating-the-circularization-of-apollo-in-kerbal-with-python-krpc-and-physics/","title":"Automating the circularization of Apollo in Kerbal with python, KRPC, and Physics"},{"content":"Introduction One of the most popular and widely known strange attractors is the Rössler strange attractor. We will explore this mathematical object using Python in a series of blog posts. Before we go into any detail about what it is, what is a strange attractor, and so on, let\u0026rsquo;s first take a quick look at it using some simple Python code.\nSimple Integration using Euler\u0026rsquo;s method This is a quick intro, and hopefully part of a longer series on using Python to analyze the Rössler Strange Attractor.\nWe begin by importing the basic libraries that we will use as follows:\n1 2 3 4 5 import numpy as np import matplotlib %matplotlib inline import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D Define Rössler System Next we define a python function which will define the Rössler equations for us to integrate and graph. The Rössler equestions are\n$$ \\dot{x} = -y -z $$ $$ \\dot{y} = x + ay $$ $$ \\dot{z} = b + z(x-c) $$\nThey function over a number of parameters, something we will explore in more detail in future posts. For now, let\u0026rsquo;s just pick a pretty common set of parameters:\n1 2 3 4 5 def rossler(x, y, z, a=0.2, b=0.2, c=5.7): x_dot = - y - z y_dot = x + a*y z_dot = b + z*(x-c) return x_dot, y_dot, z_dot Numerical Integration These equations can\u0026rsquo;t be solved for non-trivial sets of parameters, so we must integrate them numerically. Here, we use a very simpler Euler\u0026rsquo;s method of integration to get us out the door. This method is never a good idea to use for non-trivial applications, but is a quick easy way to get started.\nEuler\u0026rsquo;s method makes use of the definition of a derivative:\n$$\\dot{x} = \\frac{dx}{dt} = \\lim_{\\Delta \\rightarrow 0} \\frac{\\Delta x}{\\Delta t} $$\nTo create a simplistic way to numerical integrate where:\n$$ \\Delta x = \\dot{x} \\Delta t $$\nThis isn\u0026rsquo;t very accurate over the long term, but with small enough $\\Delta t$ can give a fair approximation in the short term. For this simple demo, we choose to let $dt = 0.05 s$ and choose to plot 1000 points for a total of 50 seconds of data.\n1 2 dt = 0.05 stepCnt = 1000 Next we set up NUMPY arrays that will hold the 1000 points for each dimension, and then set intitial values (what initial values to use will be explained in more detail in a post where we discuss the parameters as well).\n1 2 3 4 # Need one more for the initial values xs = np.empty((stepCnt + 1,)) ys = np.empty((stepCnt + 1,)) zs = np.empty((stepCnt + 1,)) 1 2 # Setting initial values xs[0], ys[0], zs[0] = (0.1, 1., 1.05) Next, we run the Euler\u0026rsquo;s method integration to populate the arrays:\n1 2 3 4 5 6 7 # Stepping through \u0026#34;time\u0026#34;. for i in range(stepCnt): # Derivatives of the X, Y, Z state x_dot, y_dot, z_dot = rossler(xs[i], ys[i], zs[i]) xs[i + 1] = xs[i] + (x_dot * dt) ys[i + 1] = ys[i] + (y_dot * dt) zs[i + 1] = zs[i] + (z_dot * dt) Plot Our final step is to plot the results so that we can see the trajectory of the Rössler system over these 50 seconds of integration.\n1 2 3 4 5 6 fig = plt.figure() ax = plt.axes(projection=\u0026#39;3d\u0026#39;) ax.plot3D(xs, ys, zs, \u0026#39;gray\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.title(\u0026#39;Rössler\u0026#39;) ","date":"2022-07-03T19:21:05-07:00","permalink":"https://tdj28.github.io/p/r%C3%B6ssler-strange-attractor-with-python-part-1/","title":"Rössler Strange Attractor with Python: Part 1"},{"content":"Testing software can often be seen as the least enjoyable part of the systems development life cycle. It is easy to see why. Developing the actual application that does the magic that the end user finds value in is indeed a very exciting core to the process. And yet, what you produce could be worthless if it doesn\u0026rsquo;t work for the user.\nSo let\u0026rsquo;s say you are working on an individual pet project. The fun part is coding it up and getting it to do something. Here, I\u0026rsquo;m going to use a simple Flask app that serves up some Wikipedia page count stats as an example. As a developer adds layers and new features and new bells and whistles and pipes and extension cords to their project, things which they haven\u0026rsquo;t developed in months or years can break, and if they don\u0026rsquo;t have a good testing suite in place, they won\u0026rsquo;t catch it and will serve out a degraded experience. That\u0026rsquo;s a dis-service to their users, but even to themselves, whose hard work has gone to waste because that piece of the project, which they might have spent many hours working on, might as well not even exist if it doesn\u0026rsquo;t work. Worse yet, if they break their app entirely, it could take hours and hours of searching for a bug.\nIf instead there is a systematic testing suite in place, each incremental change can be subjected to a battery of tests that help ensure that it is safe to continue to the next incremental change. This should not be the developer clicking through their app and making sure everything looks good. There are many awesome tools in place that allow for quick automated testing. Here, we provide a walk through to jump start a web developer on using Python and the selenium and behave libraries to provide a full browser test of their site.\nSelenium Selenium is a Python library that automates testing the final product of a webpage. Selenium integrates with various browsers and replicates the behavior that you expect from your users, all while measuring the response to that behavior against your expectations for how the site, when in its optimal state, should respond. It is as easy to install as\n1 pip install selenium nose behave Let\u0026rsquo;s start with a very simple example. Suppose you have webpage that simply serves this up:\n1 2 3 4 5 6 7 8 \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;WikiViz\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 class=\u0026#34;example\u0026#34;\u0026gt;Hello World!\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Even if you didn\u0026rsquo;t want to get fancy and set up a nice file structure and orchestrate your testing with tools such as behave and nose, you could simply create a very straight-forward python script that runs the selenium testing you need to ensure that your page is serving the expected content.\nIf you are running these locally on your laptop, the following step isn\u0026rsquo;t necessary, you will just need to make sure you have Firefox installed. Selenium works with other browsers, but to keep this simple at first, we will focus on Firefox.\nHeadless on Ubuntu (skip this if using a Mac) Skip this section if you don\u0026rsquo;t plan to run this on a remote linux-based server. In this example, I\u0026rsquo;m using my Cloud 9 IDE account which serves up containers with Ubuntu as the OS.\nFirst we need to isntall the Xvfb package:\n1 sudo apt-get install firefox xvfb and the PyVirtualDisplay python library:\n1 pip install selenium nose behave PyVirtualDisplay Finally, you\u0026rsquo;ll need the geckodriver (they don\u0026rsquo;t include md5sums or rsa hashes to confirm authenticity, but use github to host and release which comes with a great deal of trust):\n1 2 3 4 wget https://github.com/mozilla/geckodriver/releases/download/v0.21.0/geckodriver-v0.21.0-linux64.tar.gz tar zxvf geckodriver-v0.21.0-linux64.tar.gz chmod a+x geckodriver sudo mv geckodriver /usr/local/bin/ Our first selenium assertion Now we are ready for our first test. This test will do two things. First, it will cause selenium to try to access the webpage. If it can\u0026rsquo;t do so, it will fail. This means that a selenium tests automatically tells you whether or not your page is up and running right out of the box. Secondly, the test will assert that the very simple webpage example shown above is setting the title to \u0026lsquo;WikiViz\u0026rsquo; as it should be.\nIf you are using the headless on your remote server, your code will look like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/usr/bin/env python from pyvirtualdisplay import Display from selenium import webdriver display = Display(visible=0, size=(1000, 1000)) display.start() driver = webdriver.Firefox() driver.get(\u0026#39;http://localhost:8080/\u0026#39;) assert driver.title == \u0026#39;WikiViz\u0026#39; driver.quit() display.stop() If you are running this in a local environment, such as on a Mac, and you like to watch selenium open the browser and walk through the tests, then your code would look like this:\n1 2 3 4 5 6 7 from selenium import webdriver base_url = \u0026#34;http://localhost:8080\u0026#34; driver = webdriver.Firefox() driver.get(base_url) assert driver.title == \u0026#34;WikiViz\u0026#34; Yes, it is that easy! OK, well, at least for the title. As your page gets more complicated, so will your testing, but overall the spirit of simplicity in the selenium testing scheme carries over. It\u0026rsquo;s the complexities of html/javascript that you will eventually need to worry about. But we will keep things simple here.\nExpanding the tests In order to make our example just a little more interesting, and give selenium something to do rather than just observe, let\u0026rsquo;s add a button to our simple home page that takes us to another page. Add the following just after the h1 entry:\n1 2 3 \u0026lt;form\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;results\u0026#34; onclick=\u0026#34;window.location.href=\u0026#39;/results\u0026#39;\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; In this next section, I\u0026rsquo;m going to make the tests a little more complicated in structure, but for a good payoff. If you like to keep your tests in a single script as above, you can simply expand on that script. Here, I will show a simple example using nose and behave libraries. This example was inspired by a slightly more complicated (and well done) example (which doesn\u0026rsquo;t have the headless feature) here, but meant to be easier to dive into for those new to testing with selenium.\nCreate a file structure that looks as follows:\n1 2 3 4 5 6 7 8 9 10 (env) tdj28:~/workspace/selenium_tests (master) $ tree . . ├── features │ ├── __init__.py │ ├── pages │ │ └── __init__.py │ └── steps │ └── __init__.py └── simple_examples └── xvfb_example.py here\u0026rsquo;s some bash to make that happen:\n1 2 3 4 5 6 7 mkdir -p selenium_tests/features mkdir selenium_tests/pages mkdir selenium_tests/steps mkdir selenium_tests/simple_examples touch selenium_tests/features/__init__.py touch selenium_tests/pages/__init__.py touch selenium_tests/steps/__init__.py The simple_examples directory is where I put in some simple scripts for testing a few features (or writing this blog post), but you definitely don\u0026rsquo;t want something that sloppy in a production version of your code.\n(Google init python files if you don\u0026rsquo;t know what those __init__.py files are for.)\nStep One: Define the Behavior BDD development would have us define behaviors we expect of the site before we even begin to develop site. That\u0026rsquo;s a good place to start for selenium testing as well, where if these definitions are pre-existing, we can use them as our starting point. If not, we can easily create our own.\nIn the features directory, create a file called clickbutton.feature with the following content:\n1 2 3 4 5 6 7 8 Feature: ClickButton Scenario: Click Away From Homepage Given I navigate to the Home page And I see that the pagename is WikiViz And I see the header \u0026#34;Hello World!\u0026#34; When I click the button Then I see that the pagename is no longer WikiViz This is the beauty of the behave library and behavior based testing in general. We can use everyday language to describe our user\u0026rsquo;s experience, and then translate that in to tests as we will see shortly. This enables non-technical folks to contribute tests.\nNote that in the above tests, the last line entry is a bit of a cheat on my part. The test really should, in that line, confirm in some way that it made it to the results page. So don\u0026rsquo;t do what I did there! If for example you click on the button and it takes you to a 404 error page, your test will still pass. I take that shortcut here only to keep this jump start example focused on a single simple html page.\nStep Two: Turn the Behaviors Into Steps In the steps folder, create a file called clickbutton_step.py. Note that it has the same prefix clickbutton as the corresponding features file we created above, that is required for each feature file. The contents of this file will have:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from nose.tools import assert_equal, assert_not_equal, assert_true from selenium.webdriver.common.by import By @given(\u0026#39;I navigate to the Home page\u0026#39;) def step_impl(context): context.home_page.navigate(\u0026#34;http://localhost:8080/\u0026#34;) @given(\u0026#39;I see that the pagename is WikiViz\u0026#39;) def step_impl(context): assert_equal(context.home_page.get_page_title(), \u0026#34;WikiViz\u0026#34;) @given(\u0026#39;I see the header \u0026#34;Hello World!\u0026#34;\u0026#39;) def step_impl(context): assert_equal(context.home_page.get_page_header(), \u0026#34;Hello World!\u0026#34;) @when(\u0026#39;I click the button\u0026#39;) def step_impl(context): context.home_page.click_button() @then(\u0026#39;I see that the pagename is no longer WikiViz\u0026#39;) def step_impl(context): assert_not_equal(context.home_page.get_page_title(), \u0026#34;WikiViz\u0026#34;) So the pattern here is starting to become clear:\n1 User experience ---\u0026gt; Steps ---\u0026gt; Page functions We have chosen to create the steps before the page functions here because that is a natural way to proceed. The steps will tell you what functions you need to create, and then afterwards, create those functions with the narrow scope defined by the steps.\nStep Three: Turn the Steps Into Implemented Functions Let\u0026rsquo;s stick with our simple home page as detailed above. We can look behind the curtain of this page by using our browser\u0026rsquo;s developer console. On Chrome, for example, it is in settings \u0026gt; More Tools \u0026gt; Developer Tools. This gives you many great tools to inspect the code and help you locate elements in your code. For our simple example, that isn\u0026rsquo;t so necessary, but as this example is expanded to include javascript which makes the page more dynamic, these developer tools become very useful. In this screenshot below, for example, we see how it helps us identify elements of the code.\nThese elements become key as we find key elements to use in our testing.\nIn the pages folder, create a file called home_page.py with the following content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from selenium.webdriver.common.by import By from browser import Browser class HomePage(Browser): # Home Page Actions def navigate(self, address): self.driver.get(address) def get_page_title(self): return self.driver.title def get_page_header(self): return self.driver.find_element_by_class_name(\u0026#34;example\u0026#34;).text def click_button(self, *locator): self.driver.find_element_by_class_name(\u0026#34;button\u0026#34;).click() Because the source code of our simple page assigns the first and only h1 header by giving it the class \u0026ldquo;example\u0026rdquo;, that makes it easy to find_element_by_class_name. If in fact we had multiple h1 elements with that class, we would have to use the plural find_elements_by_class_name instead, which returns a list that we could submit for testing (e.g. we would know what the first h1 should be, the second, etc.).\nSo for our homepage, we have created the HomePage class, which inherits the Browser class we created previously, and we define the actions we will need on this page for testing. When we add other pages to our example web app in later blog entries, we will create a new file in the pages folder, one for each page. In this way, we keep the details nice and cleanly compartmentalized. If we have a subset of pages that share much in common, we can create a class for that subset, and then have individual pages inherit that class, and so on.\nAlso note that there are multiple ways we can identify elements, including xpath. In this file, we add four tools associated with our homepage. The first is a navigate function, the second returns the pagename, the third returns the h1 element, and the fourth clicks on the button we just added that will send us to a new page.\nStep Four: Define The Browser Before we can make use of the defined behaviors and expectations, we need to set up our browser and display (for headless).\nCreate the file features/browser.py with the following content if you want selenium to run on your laptop or desktop in a way that you can watch it run through the tests:\n1 2 3 4 5 6 7 8 9 10 11 from selenium import webdriver class Browser(object): driver = webdriver.Firefox() driver.implicitly_wait(15) driver.set_page_load_timeout(10) driver.maximize_window() def close(context): context.driver.close() Here, I\u0026rsquo;ve made the page timeout ten seconds because if my page is taking longer than ten seconds to load, to me that is a pretty good reason for it to fail. I would actually recommend you being more generous than not, and instead build in timing features to test for load time rather than have the tests timeout as an indication that your page is slow, but again, this is meant to be a baseline example.\nFor those wishing to use headless testing (on a remote server, no live browser to watch), the above file will look like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from selenium import webdriver from pyvirtualdisplay import Display # Comment this out for live browser class BDisplay(object): # Comment this out for live browser display = Display(visible=0, size=(1000, 1000)) # Comment this out for live browser display.start() # Comment this out for live browser def stop(context): # Comment this out for live browser context.display.stop() # Comment this out for live browser class Browser(object): driver = webdriver.Firefox() driver.implicitly_wait(15) driver.set_page_load_timeout(10) driver.maximize_window() def close(context): context.driver.close() Step Five: Putting it all together Finally, we need to have an environment definition to tie everything together. This will tell behave what page classes to import and tell it what to do before and after the tests are ran.\nIf the features folder, create a file called environment.py with the content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from selenium import webdriver from browser import BDisplay # Comment this out for live browser from browser import Browser from pages.home_page import HomePage def before_all(context): context.display = BDisplay() # Comment this out for live browser context.browser = Browser() context.home_page = HomePage() def after_all(context): context.browser.close() context.display.stop() # Comment this out for live browser In the folder above features, assuming you\u0026rsquo;ve installed behave via pip already, simply run behave. If all goes well, we get an output that indicates the tests ran and their pass/fail status:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 (env) tdj28:~/workspace/selenium_tests (master) $ behave Feature: ClickButton # features/clickbutton.feature:1 Scenario: Click Away From Homepage # features/clickbutton.feature:3 Given I navigate to the Home page # features/steps/clickbutton_steps.py:4 0.112s And I see that the pagename is WikiViz # features/steps/clickbutton_steps.py:8 0.005s And I see the header \u0026#34;Hello World!\u0026#34; # features/steps/clickbutton_steps.py:12 0.046s When I click the button # features/steps/clickbutton_steps.py:16 0.112s Then I see that the pagename is no longer WikiViz # features/steps/clickbutton_steps.py:20 0.009s 1 feature passed, 0 failed, 0 skipped 1 scenario passed, 0 failed, 0 skipped 5 steps passed, 0 failed, 0 skipped, 0 undefined Took 0m0.283s Find the code here For your convenience, the selenium code is collected here.\n","date":"2018-08-26T19:21:05-07:00","permalink":"https://tdj28.github.io/p/headless-python-selenium-with-behave/","title":"Headless Python Selenium With Behave"},{"content":"Heat maps have become quite common in data science, and in science in general, and for good reason. For those who can see them, they succinctly summarize patterns in data. There are many color schemes used in the wild, with the Rainbow scheme being one of the more common ones.\nA group at Pacific Northwest National Lab has recently published a PLOS One article (hat tip Scientific American article) outlining their efforts to use mathematical modeling and the science of vision to create color maps for plots such that the exact same information is conveyed to those with and without color blindness.\nThe goal of this blogpost is to demonstrate a quick way you can utilize their results in your own graphs (if you use Python). Interested readers can follow the links above for more detail on their methodology. What follows below is enough to get you off the ground using their Cividis colormap right away. Here is a sample of the difference one can get using Cividis:\nThe sample above is just a starting point. More sophisticated examples of the use of Cividis can be found in the corresponding library and the above reference article.\nInstalling the Library \u0026lt;p\u0026gt;UPDATE Sep 13 2018:\u0026lt;/p\u0026gt; The PR has been merged and you can just install with pip install cmaputil without having to pull the branch.\nPNNL\u0026rsquo;s cmaputil library can be downloaded from their github page or installed via pip install cmaputil. However, for the use cases I\u0026rsquo;m showing here, that won\u0026rsquo;t quite work. This PR shows why. If that PR has been merged, or an alternative fix merged, you should be able to use the library from the source. If not, here is a quick work around:\n1 2 3 4 5 git clone https://github.com/tdj28/cmaputil git checkout -b patch-1 git pull cd cmaputil python setup.py install Assuming that works, you can now use the camputil library to use optimized Cividis colormaps.\nUsing Cividis We would start by importing the key libraries:\n1 2 3 import cmaputil as cmu import cmaputil.cvdutil as cvu import scipy Next, add the following section to your code. It modifies the viridis colormap that comes with matplotlib and runs some optimizations on it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # Input colormap nam cmap = \u0026#39;viridis\u0026#39; # Optimize rgb1, jab1 = cmu.get_rgb_jab(cmap) # Original colormap rgb2, jab2 = cmu.get_rgb_jab(cvu.get_cvd(rgb1)) # CVD colormap jab3 = cmu.make_linear(jab2) # Uniformize hue (a\u0026#39; vs. b\u0026#39;) #print(jab3) _, jab4 = cmu.correct_J(jab3) # Linearize J\u0026#39; # Convert back to sRGB rgb4 = cmu.convert(jab4, cmu.CSPACE2, cmu.CSPACE1) rgb4 = np.clip(rgb4, 0, 1) # Resimulate CVD in case corrections took the map outside CVD-safe space rgb4 = cvu.get_cvd(rgb4) # Resimulate CVD in case corrections took the map outside CVD-safe space rgb4 = cvu.get_cvd(rgb4) colors = [] for j in range(len(rgb4[0])): colors.append( (rgb4[0][j], rgb4[1][j], rgb4[2][j]) ) # Convert to matplotlib colormap cividis = mpl.colors.LinearSegmentedColormap.from_list(\u0026#34;\u0026#34;, colors) Finally, you can easily incorporate it into your matplotlib plots as in the following example:\n1 2 3 4 5 6 7 8 plt.clf() fig = plt.figure(figsize=(9, 9)) plt.title(\u0026#39;Cividis Heatmap\u0026#39;) plt.ylabel(\u0026#39;y\u0026#39;) plt.xlabel(\u0026#39;x\u0026#39;) im = plt.imshow(heatmap, extent=extent, cmap=cividis, origin=\u0026#39;lower\u0026#39;) fig.colorbar(im) plt.savefig(\u0026#39;Cividis Heatmap\u0026#39;) We can compare the rainbow version of the above plot:\nwith the Cividis version:\nIt certainly may be a matter of preference for most folks. I find the Cividis version more appealing, for example, because although the dark reds pop to me and pinpoint \u0026ldquo;hot spots\u0026rdquo; perhaps a little more quickly, they appear to be almost detached from the rest of the heat blob. In the Cividis mapping, as in other mappings which have only two colors, the spectrum is clearer. Given that using Cividis ensures that any color blind readers will be getting the same information as I see seals the deal for me and makes Cividis my go to choice for colormaps going forward.\nConclusions Cividis is an optimized version of the viridis colormap which optimizes for both the ability to discern patterns for those with normal vision and the ability of the color blind to see the exact same thing.\nCividis works in Python and they also have an R library. This quick blog note shows how to use it right now, but I anticipate and hope that it will become obsolete and matplotlib will incorporate Cividis and PNNL\u0026rsquo;s work on this. COMSOL Multiphysics, a finite element analysis software package, for example, will be doing so.\nYou can find a standalone python script which I used to create these plots using Cividis here\n","date":"2018-08-19T19:21:05-07:00","permalink":"https://tdj28.github.io/p/using-pnnls-new-cividis-colormap-in-data-science-to-make-accessible-heat-maps/","title":"Using PNNL's new Cividis colormap in data science to make accessible heat maps"},{"content":"Exploring Wikipedia page counts via additive models for seasonality decomposition Introduction Wikipedia, in addition to being quite valuable as a starting point for many student essays (as somebody who has spent time in front of the classroom, I can\u0026rsquo;t emphasize the word starting enough here), can provide some sociological insight by providing page count hits. In this brief blog entry, we will explore two python-ready implementations of additive models for seasonality decomposition in the context of the following topic:\nIs there any seasonality with wikipedia page hits? For this question, we will look at a few keywords which we think are quite often used by school students in researching for common essay questions (e.g. Abraham_Lincoln) and other keywords which are less likely to be commonly used by students for essays at any level, for example, the American telecommunications company Sprint. This blog entry is posted in the form of a Jupyter Notebook so that readers can recreate the findings and pursue further questions more easily.\nWe use two additive model implementations, one from a library called statsmodel and another from a library developed by Facebook called prophet.\nAdditive models for seasonality decomposition, a quick primer Let\u0026rsquo;s begin by creating a artificial time series.\nBefore importing pandas, numpy, etc., I want to turn off some typical warnings that we see from these libraries. This is just to keep the output looking clean for presentation purposes.\n1 2 3 4 import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;numpy.dtype size changed\u0026#34;) warnings.filterwarnings(\u0026#34;ignore\u0026#34;, message=\u0026#34;numpy.ufunc size changed\u0026#34;) warnings.filterwarnings(\u0026#34;ignore\u0026#34;,category=FutureWarning) Next, we import all of the great Python libraries. Here, for seasonal decomposition we are using statsmodel and Facebook\u0026rsquo;s Prophet.\n1 2 3 4 5 6 7 8 9 10 11 12 import pandas as pd import numpy as np import matplotlib.pylab as plt import datetime %matplotlib inline from matplotlib.pylab import rcParams import seaborn as sns from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.seasonal import seasonal_decompose from fbprophet import Prophet rcParams[\u0026#39;figure.figsize\u0026#39;] = 16, 7 Next, let\u0026rsquo;s create a fake data set.\n1 2 3 4 5 entries = 365 * 2 # Two years worth of days df = pd.DataFrame(np.random.randn(entries)).cumsum() df.head() plt.plot(df[0].tolist()) plt.show() Our brains have pattern-matching algorithms trained over billions of years, so even though the above data set is a random walk, we may see patterns there anyway. If we run it again, those patterns will change even though we were using the exact same algorithm to generate them. This of course doesn\u0026rsquo;t imply that finding patterns in data is an empty task, there is very often patterns that are legitimate. The point here is to show how the additive model works by throwing something known to be random at it. Just to show, again, how entirely random the generated dataset is, let\u0026rsquo;s throw a bunch in the same graph:\n1 2 3 4 5 6 7 8 9 def rwts(entries=100): _df = pd.DataFrame(np.random.randn(entries)).cumsum() start = datetime.datetime.strptime(\u0026#34;20160101\u0026#34;, \u0026#34;%Y%d%m\u0026#34;) dates_generated = [start + datetime.timedelta(days=x) for x in range(0, entries)] dtse = pd.Series(dates_generated) _df[\u0026#39;ds\u0026#39;] = dtse _df = _df.rename(columns={0: \u0026#34;y\u0026#34;}) _df.index = _df[\u0026#39;ds\u0026#39;] return _df 1 2 3 4 5 6 7 entries = 365 * 2 # Two years worth of days rwdf = [] for i in range(0, 10): rwdf.append(rwts(entries)) plt.plot(rwdf[i][\u0026#39;y\u0026#39;]) #.tolist()) plt.show() Next, let\u0026rsquo;s see what happens when we apply the statsmodel implementation of the additive model to a random walk dataset:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def additive_seasonal_plot(_df, frequency=365, title=\u0026#34;I forgot to tinclude a title.\u0026#34;): fig, ax = plt.subplots() plt.title(title) decompositionm = seasonal_decompose(_df, freq=frequency, model=\u0026#39;additive\u0026#39;) trendm = decompositionm.trend seasonalm = decompositionm.seasonal residualm = decompositionm.resid plt.subplot(411) plt.plot(_df, label=\u0026#39;Original\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(412) plt.plot(trendm, label=\u0026#39;Trend\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(413) plt.plot(seasonalm,label=\u0026#39;Seasonality\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(414) plt.plot(residualm, label=\u0026#39;Residuals\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.tight_layout() 1 additive_seasonal_plot(rwts(365*2)[\u0026#39;y\u0026#39;], frequency=365, title=\u0026#34;Random Walk\u0026#34;) Now let\u0026rsquo;s try that with Prophet:\n1 2 3 4 5 6 7 8 def prophet_seasonal_plot(_df, frequency=365, mytitle=\u0026#34;I forgot to tinclude a title.\u0026#34;): _m = Prophet(yearly_seasonality = True, daily_seasonality=False, weekly_seasonality = True, mcmc_samples = 0, seasonality_prior_scale=50) _df[\u0026#39;y\u0026#39;].plot(title=mytitle) _m.fit(_df) _future = _m.make_future_dataframe(periods=365) _forecast = _m.predict(future) _m.plot_components(_forecast); _m.plot(_forecast); 1 prophet_seasonal_plot(rwts(365*2), frequency=365, mytitle=\u0026#34;Random Walk\u0026#34;) Prophet makes nicer graphs out of the box, for sure. It also has a nice way to show uncertainty. And looking at the above, you might very well believe that indeed we have found some sort of phenomenon that seems to peak in late winter and spring, and on Thursday for some reason. It would be a pretty safe bet to bet that the above trend would peak in the months following the last dataset we have, right? Wrong, of course, because as you know, this dataset was generated entirely randomly. The lesson here is not that this method for modeling seasonality and forecasting is wrong, as indeed they work exactly as they are designed to, but instead that we need to be quite careful not to so readily believe what they say. This is the giant grain of salt I\u0026rsquo;m putting on your plate before getting to the fun stuff.\nTo make this point entirely visual, let\u0026rsquo;s do the same analysis as above, but instead of just using two years worth of fake data, let\u0026rsquo;s create three years, but only feed the additive model two years, and see how it does in the third year.\n1 2 3 4 5 6 7 8 9 10 11 df1 = rwts(365*3) mask1 = (df1[\u0026#39;ds\u0026#39;] \u0026gt;= \u0026#39;20160101\u0026#39;) \u0026amp; (df1[\u0026#39;ds\u0026#39;] \u0026lt; \u0026#39;20180101\u0026#39;) df2 = df1.loc[mask1] mask2 = (df1[\u0026#39;ds\u0026#39;] \u0026gt;= \u0026#39;20180101\u0026#39;) \u0026amp; (df1[\u0026#39;ds\u0026#39;] \u0026lt; \u0026#39;20190101\u0026#39;) df3 = df1.loc[mask2] _m = Prophet(yearly_seasonality = True, daily_seasonality=False, weekly_seasonality = True, mcmc_samples = 0, seasonality_prior_scale=50) _m.fit(df2) _future = _m.make_future_dataframe(periods=365) _forecast = _m.predict(future) _m.plot(_forecast); That looks great, that stock is about to go up like crazy, let\u0026rsquo;s dump our entire life-savings into that stock!\n1 2 _m.plot(_forecast); df3[\u0026#39;y\u0026#39;].plot(color=\u0026#39;Red\u0026#39;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7fbe8d1048\u0026gt; Oops.\nWikipedia Page hit stats Previously, in order to get a nice way to access Wikipedia page hit data, I would have suggsted the R library wikipediatrend. The situation has evolved such that wikipediatrend is not functional (see this link for more details). Fortunately, Wikimedia themselves have released an api that has a python library.\n1 2 3 from mwviews.api import PageviewsClient p = PageviewsClient(\u0026#39;blog\u0026#39;) Although there is a repository of old pageviews pre-dating July 2015, the API only provides dates from July 2015 forward. Let\u0026rsquo;s get three years worth of data from July 2015 to July 2018. The following command outputs a dictionary with items of the form:\n1 datetime.datetime(2015, 12, 21, 0, 0): {\u0026#39;Lincoln\u0026#39;: 503}, when we make a call to extract page hit counts for the topic \u0026lsquo;Abraham Lincoln\u0026rsquo;.\n1 dictLincoln = p.article_views(\u0026#39;en.wikipedia\u0026#39;, [\u0026#39;Abraham Lincoln\u0026#39;], granularity=\u0026#39;daily\u0026#39;, start=\u0026#39;20150701\u0026#39;, end=\u0026#39;20180701\u0026#39;); 1 dfLincoln = pd.DataFrame.from_dict(dictLincoln).transpose() 1 2 3 dfLincoln.index = pd.to_datetime(dfLincoln.index) dfLincoln = dfLincoln.rename(columns={\u0026#34;Abraham_Lincoln\u0026#34;: \u0026#34;Daily Page Counts\u0026#34;}) dfLincoln.head() Daily Page Counts 2015-07-01 15223 2015-07-02 15137 2015-07-03 17291 2015-07-04 19697 2015-07-05 18149 Do we see any trends just by glancing at the plot of these page hits over the three year period from July 2015 to July 2018? Initially we may see a few, such as that the page hits seem to go up after January, and have some unusual peaks in late 2016 and early 2017 (very likely, and precisely as we will see later, related to the election and inauguration of that period).\n1 dfLincoln.plot(title=\u0026#34;Abraham Lincoln Wiki Page Hits\u0026#34;) \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7fc8603a90\u0026gt; In fact, let\u0026rsquo;s plot key dates on that plot, such as the US election of 2016, inauguration of 2017, President\u0026rsquo;s Day holidays, and the 4th of July. These key dates explain almost all of the spikes.\n1 2 3 4 5 6 7 ax = dfLincoln.plot(title=\u0026#34;Abraham Lincoln Wiki Page Hits + Key dates\u0026#34;, linewidth=3.0); important_dates = [\u0026#39;2016-11-08\u0026#39;, \u0026#39;2017-01-20\u0026#39;, \u0026#39;2016-02-15\u0026#39;, \u0026#39;2017-02-20\u0026#39;, \u0026#39;2018-02-19\u0026#39;, \u0026#39;2015-07-04\u0026#39;, \u0026#39;2016-07-04\u0026#39;, \u0026#39;2017-07-04\u0026#39;, \u0026#39;2018-07-04\u0026#39;] for xc in important_dates: plt.axvline(x=xc, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.set_xlim(pd.Timestamp(\u0026#39;2016-01-01\u0026#39;), pd.Timestamp(\u0026#39;2018-03-01\u0026#39;)) ax.set_ylim(0, 90000) (0, 90000) Next we apply statsmodel\u0026rsquo;s implementation of the additive model for seasonal decomposition:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 fig, ax = plt.subplots() plt.title(\u0026#39;Seasonal trend for Wiki Page Counts: Abraham Lincoln\u0026#39;) decompositionm = seasonal_decompose(dfLincoln, freq=365, model=\u0026#39;additive\u0026#39;) trendm = decompositionm.trend seasonalm = decompositionm.seasonal residualm = decompositionm.resid plt.subplot(411) plt.plot(dfLincoln, label=\u0026#39;Original\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(412) plt.plot(trendm, label=\u0026#39;Trend\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(413) plt.plot(seasonalm,label=\u0026#39;Seasonality\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(414) plt.plot(residualm, label=\u0026#39;Residuals\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.tight_layout() This model has identified a trend increase in election year 2016, which seems to make sense. However, it overfit the spike seen around election day 2016, and we can see the result of that overfit in the residuals which dip extremely a year before and after election day. In a future blog post, we will look at working around this by marking the key dates as holidays so that the algorithm doesn\u0026rsquo;t overfit those days. The seasonality trend has also found something pretty expected, which is that hits on that page seem to breathe with the typical American school year. That is, we can see in the seasonal trends that there are many students using Wikipedia (hopefully as a starting point and not a primary resource) for essay papers and homework answers on Abraham Lincoln.\n1 2 3 4 5 6 7 8 9 10 11 12 ax = seasonalm.plot(title=\u0026#34;Abraham Lincoln Wiki Page Hits Seasonal Trend + Key dates + Shaded School Year\u0026#34;, linewidth=3.0); important_dates = [\u0026#39;2016-11-08\u0026#39;, \u0026#39;2017-01-20\u0026#39;, \u0026#39;2016-02-15\u0026#39;, \u0026#39;2017-02-20\u0026#39;, \u0026#39;2018-02-19\u0026#39;, \u0026#39;2015-12-25\u0026#39;, \u0026#39;2016-12-25\u0026#39;, \u0026#39;2017-12-25\u0026#39;, \u0026#39;2015-07-04\u0026#39;, \u0026#39;2016-07-04\u0026#39;, \u0026#39;2017-07-04\u0026#39;, \u0026#39;2018-07-04\u0026#39;] for xc in important_dates: ax.axvline(x=xc, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.axvspan(pd.Timestamp(\u0026#39;2015-09-01\u0026#39;), pd.Timestamp(\u0026#39;2016-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2016-09-01\u0026#39;), pd.Timestamp(\u0026#39;2017-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2017-09-01\u0026#39;), pd.Timestamp(\u0026#39;2018-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.set_xlim(pd.Timestamp(\u0026#39;2015-07-01\u0026#39;), pd.Timestamp(\u0026#39;2018-03-01\u0026#39;)) ax.set_ylim(-10000, 40000) (-10000, 40000) Zooming in on the residuals, we see again how the overfitting to the special events (election and inauguration) hurt the accuracy of the models.\n1 2 3 4 5 6 ax = residualm.plot(title=\u0026#34;Abraham Lincoln Wiki Page Hits Seasonal Residuals + Key dates\u0026#34;, linewidth=3.0); important_dates = [ \u0026#39;2016-01-20\u0026#39;, \u0026#39;2016-11-08\u0026#39;, \u0026#39;2017-01-20\u0026#39;, \u0026#39;2017-11-08\u0026#39;] for xc in important_dates: ax.axvline(x=xc, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.set_xlim(pd.Timestamp(\u0026#39;2015-12-01\u0026#39;), pd.Timestamp(\u0026#39;2018-02-01\u0026#39;)) ax.set_ylim(-40000, 40000) (-40000, 40000) Now we have stated a hypothesis without backing it up, namely that the seasonality over the year in page hits for the Wikipedia entry for Abraham Lincoln reflects the patterns of the American school year. This is pretty impossible to prove without data that is impossible or not practical to obtain, namely we would have to have the IP addresses of all those page hits and tie those in to schools and households with students, etc. Never going to happen. But we can strengthen our confidence in our hypothesis by testing it in the negative, that is, what will we see if we pick a topic which we do not expect to be tied to school-year patterns, a topic students would rarely ever look up for school work? Let\u0026rsquo;s try the American telecom Sprint.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 def get_wiki_df(topic, ymin, ymax): _dict = p.article_views(\u0026#39;en.wikipedia\u0026#39;, [topic], granularity=\u0026#39;daily\u0026#39;, start=\u0026#39;20150701\u0026#39;, end=\u0026#39;20180701\u0026#39;); _df = pd.DataFrame.from_dict(_dict).transpose() _df.index = pd.to_datetime(_df.index) _df = _df.rename(columns={topic: \u0026#34;Daily Page Counts\u0026#34;}) ax = _df.plot(title=\u0026#34;{} Wiki Page Hits + Key dates + Shaded School Year\u0026#34;.format(topic), linewidth=3.0); important_dates = [\u0026#39;2016-11-08\u0026#39;, \u0026#39;2017-01-20\u0026#39;, \u0026#39;2016-02-15\u0026#39;, \u0026#39;2017-02-20\u0026#39;, \u0026#39;2018-02-19\u0026#39;, \u0026#39;2015-12-25\u0026#39;, \u0026#39;2016-12-25\u0026#39;, \u0026#39;2017-12-25\u0026#39;, \u0026#39;2015-07-04\u0026#39;, \u0026#39;2016-07-04\u0026#39;, \u0026#39;2017-07-04\u0026#39;, \u0026#39;2018-07-04\u0026#39;] for xc in important_dates: ax.axvline(x=xc, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.axvspan(pd.Timestamp(\u0026#39;2015-09-01\u0026#39;), pd.Timestamp(\u0026#39;2016-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2016-09-01\u0026#39;), pd.Timestamp(\u0026#39;2017-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2017-09-01\u0026#39;), pd.Timestamp(\u0026#39;2018-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.set_xlim(pd.Timestamp(\u0026#39;2015-07-01\u0026#39;), pd.Timestamp(\u0026#39;2018-03-01\u0026#39;)) ax.set_ylim(ymin, ymax) return _df def get_wiki_seasonal(topic, ymin, ymax): _dict = p.article_views(\u0026#39;en.wikipedia\u0026#39;, [topic], granularity=\u0026#39;daily\u0026#39;, start=\u0026#39;20150701\u0026#39;, end=\u0026#39;20180701\u0026#39;); _df = pd.DataFrame.from_dict(_dict).transpose() _df.index = pd.to_datetime(_df.index) _df = _df.rename(columns={topic: \u0026#34;Daily Page Counts\u0026#34;}) decompositionm = seasonal_decompose(_df, freq=365, model=\u0026#39;additive\u0026#39;) seasonalm = decompositionm.seasonal ax = seasonalm.plot(title=\u0026#34;{} Wiki Page Hits Seasonal Trend + Key dates + Shaded School Year\u0026#34;.format(topic), linewidth=3.0); important_dates = [\u0026#39;2016-11-08\u0026#39;, \u0026#39;2017-01-20\u0026#39;, \u0026#39;2016-02-15\u0026#39;, \u0026#39;2017-02-20\u0026#39;, \u0026#39;2018-02-19\u0026#39;, \u0026#39;2015-12-25\u0026#39;, \u0026#39;2016-12-25\u0026#39;, \u0026#39;2017-12-25\u0026#39;, \u0026#39;2015-07-04\u0026#39;, \u0026#39;2016-07-04\u0026#39;, \u0026#39;2017-07-04\u0026#39;, \u0026#39;2018-07-04\u0026#39;] for xc in important_dates: ax.axvline(x=xc, color=\u0026#39;k\u0026#39;, linestyle=\u0026#39;--\u0026#39;) ax.axvspan(pd.Timestamp(\u0026#39;2015-09-01\u0026#39;), pd.Timestamp(\u0026#39;2016-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2016-09-01\u0026#39;), pd.Timestamp(\u0026#39;2017-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.axvspan(pd.Timestamp(\u0026#39;2017-09-01\u0026#39;), pd.Timestamp(\u0026#39;2018-06-01\u0026#39;), facecolor=\u0026#39;g\u0026#39;, alpha=0.3) ax.set_xlim(pd.Timestamp(\u0026#39;2015-07-01\u0026#39;), pd.Timestamp(\u0026#39;2018-03-01\u0026#39;)) ax.set_ylim(ymin, ymax) return _df 1 2 ax=get_wiki_df(\u0026#34;Sprint\u0026#34;, 0, 400) ax=get_wiki_seasonal(\u0026#34;Sprint\u0026#34;, -100, 400) The seasonality trend is scarred by incorporating a singular spike in data (the additive model tries to balance bias and variance, but obviously both can break at the same time), but we see no pattern tied to the school year. (One flaw is that the hits on the Sprint wiki page are much lower to begin with, so it isn\u0026rsquo;t as reliable of a statement.) Let\u0026rsquo;s try another topic which we suspect should breathe with the school year, US President George Washington.\n1 2 ax=get_wiki_df(\u0026#34;George Washington\u0026#34;, 0, 80000) ax=get_wiki_seasonal(\u0026#34;George Washington\u0026#34;, -20000, 60000) The trend here is less pronounced, but still evident. While we are on the topic, we present below, without commentary, a comparison on page hits between Donald Trump and Bill Clinton. I present both on the same scale to show the extraordinary difference in the magnitude of public interest in each individual.\n1 2 ax=get_wiki_df(\u0026#34;Donald Trump\u0026#34;, 0, 7000000) ax=get_wiki_seasonal(\u0026#34;Donald Trump\u0026#34;, -200000, 5000000) 1 2 ax=get_wiki_df(\u0026#34;Bill Clinton\u0026#34;, 0, 7000000) ax=get_wiki_seasonal(\u0026#34;Bill Clinton\u0026#34;, -200000, 5000000) Let\u0026rsquo;s make use of Prophet now, because it has nicer graphs and seems a little more robust in my limited usage so far. We\u0026rsquo;ve used it above, but I will walk through a little more slowly this time. First we must create a Prophet object, and fit our dataframe to Prophet such that there is a datetime column labeled ds, and the counts value column is relabeled y.\n1 2 3 4 m = Prophet(yearly_seasonality = True, weekly_seasonality = True, mcmc_samples = 0, seasonality_prior_scale=50) proLincoln = dfLincoln proLincoln[\u0026#39;ds\u0026#39;] = dfLincoln.index proLincoln.head() Daily Page Counts ds 2015-07-01 15223 2015-07-01 2015-07-02 15137 2015-07-02 2015-07-03 17291 2015-07-03 2015-07-04 19697 2015-07-04 2015-07-05 18149 2015-07-05 1 2 proLincoln = proLincoln.rename(columns={\u0026#39;Daily Page Counts\u0026#39;: \u0026#39;y\u0026#39;}) proLincoln.head() y ds 2015-07-01 15223 2015-07-01 2015-07-02 15137 2015-07-02 2015-07-03 17291 2015-07-03 2015-07-04 19697 2015-07-04 2015-07-05 18149 2015-07-05 1 m.fit(proLincoln) INFO:fbprophet.forecaster:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this. /home/d7082791602/.local/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`. elif np.issubdtype(np.asarray(v).dtype, float): \u0026lt;fbprophet.forecaster.Prophet at 0x7f7fbdf32cc0\u0026gt; 1 2 3 4 5 future = m.make_future_dataframe(periods=365) forecast = m.predict(future) forecast.head() m.plot_components(forecast); m.plot(forecast); Here we see that prophet also very nicely picks out the 2016 trend, and clearly picks out the school year trend as well as the weekday trend. In order to make these quick peaks at the data more systematic, we create and use the following function. We then use these functions to take a look at a varied array of Wikipedia pages and conjecture a bit about the results.\nThe first one we take a look at is \u0026ldquo;Christmas\u0026rdquo; which we expect, and indeed find, has a very clean seasonality. Note the weekday trend line shows a preference for some days over others, which is an artifact simply of which day the holiday fell on in the years that are used for training (since this holiday doesn\u0026rsquo;t fall on any particular weekday by design, and we only trained based on three years, those three days in occured on, or rather Christmas Eve, will see the bias. That would be Thursday/Friday 2015, Sunday/Monday 2016, and Monday/Tuesday 2017 [2016 was a leap year]).\n1 2 3 4 5 6 7 8 9 10 11 12 13 def prophet_wiki_df(topic, starting=\u0026#39;20150701\u0026#39;, ending=\u0026#39;20180804\u0026#39;): _m = Prophet(yearly_seasonality = True, daily_seasonality=False, weekly_seasonality = True, mcmc_samples = 0, seasonality_prior_scale=50) _dict = p.article_views(\u0026#39;en.wikipedia\u0026#39;, [topic], granularity=\u0026#39;daily\u0026#39;, start=starting, end=ending); _df = pd.DataFrame.from_dict(_dict).transpose() _df.index = pd.to_datetime(_df.index) _df.plot(title=\u0026#34;{} Wikipedia Page Hit Counts\u0026#34;.format(topic)) _df = _df.rename(columns={topic.replace(\u0026#34; \u0026#34;, \u0026#34;_\u0026#34;): \u0026#34;y\u0026#34;}) _df[\u0026#39;ds\u0026#39;] = _df.index _m.fit(_df) _future = _m.make_future_dataframe(periods=365) _forecast = _m.predict(future) _m.plot_components(_forecast); _m.plot(_forecast); 1 prophet_wiki_df(\u0026#39;Christmas\u0026#39;) Now let\u0026rsquo;s look at something that shouldn\u0026rsquo;t really have too much seasonality, the term eclipse. Eclipses happen on a regular basis, of course, and physics predicts them extremely accurately, but human observation of eclipses happens irregularly. Here we can see that the major eclipse event of 2017 is so strongly represented in the data that it ends of defining the very inacurate model that results.\n1 prophet_wiki_df(\u0026#39;Eclipse\u0026#39;) We can see the fading of pop culture stars, such as the declining trend with Beyonce. Again, we see that the none of the output here is very trustworthy except perhaps the overall trend. Events such as awards ceremonies, album releases, and gossip events result in spikes which then dominate the seasonality models.\n1 prophet_wiki_df(\u0026#39;Beyoncé\u0026#39;) Returning to our theme above that some wiki topics breathe with the school year, electron is surely one of those topics.\n1 prophet_wiki_df(\u0026#39;Electron\u0026#39;) Conclusions Statistical tools for seasonal decomposition are a great start in trying to predict time series which have some predictability built in. For topics that have a much greater stochastic characteristic, these tools become much more difficult to use properly. Blog entries here will continue to explore these tools and go into greater detail about the math behind them and how to use the tools in more sophisticated ways. Initial impressions from this exploration here:\nThere are some time series which are nearly perfectly predictable, such as that for the page hits on the topic of Christmas, which is an event that happens once per year and is limited in scope to a small minority of weeks in the entire year.\nOther trends are fairly predictable, such as electron, because of its association with school work and the predictable seasonality of school. Interestingly, the trends one finds with topics like this show the \u0026ldquo;hit\u0026rdquo; that the holiday break seems to have on education, where the end of year break is preceeded with a sort of winding down and then followed by a winding up, both of which are not particularly rapid.\nTrends that have an educational component, such as popular US Presidents like Abraham Lincoln, are complicated by holidays such as president\u0026rsquo;s day, elections, and inaugurations.\nTrends from pop culture, such as Beyonce, can possibly trace the popularity of a celebrity but, are complicated by things such as album releases, awards ceremonies, gossip outbreaks, etc.\nSometimes, a massive surge in interest in a topic due to some even can entirely throw off seasonal decomposition, and such events will have to be \u0026lsquo;windowed out\u0026rsquo; and interpolated to get an accurate picture of the routine behavior of the time series.\nSome topics show the decline in interest in the topic in general, or in some cases, an increasing normalization of a topic such that it gets less page views over time.\nSome topics are so random that speaking about trends and seasonality is nonsense. However, these models can easily overfit that data and give you a false sense of confidence that will inevitably be proven wrong.\n","date":"2018-08-05T19:21:05-07:00","permalink":"https://tdj28.github.io/p/exploring-wikipedia-page-count-trends/","title":"Exploring Wikipedia Page Count Trends"},{"content":"Bowtie Diagram in LaTeX For folks who have to conduct risk analysis, there are very limited choices for the creation of \u0026ldquo;bowtie diagrams\u0026rdquo;. Searching the web for examples in LaTeX comes up pretty empty. So when googling comes up empty, its time to create:\nBowtie diagram in LaTeX example\nThis code results in a diagram that looks pretty acceptable in terms of quality, or at least an acceptable starting point:\n","date":"2018-06-24T19:21:05-07:00","permalink":"https://tdj28.github.io/p/a-risk-analysis-bowtie-diagram-in-latex/","title":"A Risk Analysis Bowtie Diagram in Latex"},{"content":" UPDATE July 23 2022: The JWST has taken spectrographic readings of the TRAPPIST system. We will update this blog entry with any key findings when they are released. It will be of great interest if JWST detects atmosphere and water on any of the TRAPPIST planets in the habitable zone.\nUPDATE Aug 10 2021: An article (preprint, NASA ADS) in the Monthly Notices of the Royal Astronomical Society suggests that ultra-cool dwarf stars are potentially not as deadly as first thought. The article suggests that the flaring tends to happen at higher latitudes on the star, which would result in such flares to tend to be directed away from the planets which are located closer to the equatorial plane of the star. Should this prove to be accurate, then that would soften the skepticism we displayed in the earlier version of this blog in regards to stars of this type hosting life, although the other causes of skepticism still remain high.\nQuick overview It was recently announced that a nearby star, TRAPPIST-1, has been found to have additional planets (some were discovered prior to this study) which exist in a potentially life-hosting region around the star. [OLD TEXT] Astronomically speaking, TRAPPIST-1 is pretty close to us and thus if it does host life then it will be a promising candidate for spectrographic evidence. [SEE UPDATE ABOVE] [OLD TEXT] However, the nature of the star and the configuration of the planets means that there are some good reasons to be skeptical that any of those planets do host life. [SEE UPDATE ABOVE] Also, it is practically impossible with our current technology to visit that system with unmanned probes, let alone (hu)maned missions. The distances in space are so vast and the restrictions that the laws of Physics place on space travel are so stringent that we would probably have to completely exploit our entire planet\u0026rsquo;s resources to even have a chance to visit that solar system within 10,000 years. We are probably not alone in the universe, but we are also probably very, very rare, and we likely won\u0026rsquo;t meet any alien intelligence before we go extinct. Deeper Dive An article published in the science journale Nature (Gillon et al., Nature, 542, February 23, 2017) made many mainstream media headlines by announcing the discovery of new terestrial planets around a nearby star. CNN was pretty tame by announcing that Astronomers discover 7 Earth-sized planets orbiting nearby star in their headline. The New York Times was equally tame in announcing that 7 Earth-Size Planets Orbit Dwarf Star, NASA and European Astronomers Say. NBC went a little louder with the headline Living on the TRAPPIST-1 Planets Would Be Very Strange, which, let\u0026rsquo;s be clear, suggest to the average reader that those planets are in fact habitable, which is not confirmed yet (more on that soon). CNET was bolder with a headline \u0026lsquo;Incredible\u0026rsquo; star system could host life, but the \u0026ldquo;could\u0026rdquo; was a good word to use though it spoke nothing to the actual probability. I could win the lottery and get struck down by lightening and then a meteor could hit the ambulance carrying me to the hospital, but I\u0026rsquo;m not going to bet on it. The UK Telegraph pushed it a bit further with the headline Nasa discovers new solar system TRAPPIST-1 - where life may have evolved on three planets. The Guardian asked is there life on exoplanets orbiting Trappist-1?\nEven Fox News got in on the fun by asking a science reporter whether this was \u0026ldquo;just a big deal or a way to get the general public interseted in space exploration to insure that it\u0026rsquo;s properly funded\u0026hellip;?\u0026rdquo; in a video with the title \u0026ldquo;Could New Earth-like Planets Be Ripe for Colonization?\u0026rdquo;\nFor the record, I don\u0026rsquo;t have an issue with those headlines, as if that would matter anyway. I\u0026rsquo;m even a bit fond of that creative website that some planetary scientist have put up to host information and fan fiction related to TRAPPIST-1. The more public interest they can drum up for their work, the more grant money that will flow towards planetary and exoplanetary science.\nHowever, let\u0026rsquo;s bring this discussion back down to Earth for a moment.\nSome background on TRAPPIST-1 TRAPPIST-1 is a very small and relatively cold star that is about 40 light-years away from our solar system. TRAPPIST-1\u0026rsquo;s radial velocity is 56.3 km/s towards us, which isn\u0026rsquo;t very fast. For example, the Juno spacecraft\u0026rsquo;s maximum speed was about 73.6 km/s. The fastest speed of a (hu)manned spacecraft was achieved by Apollo 10 at about 11.08 km/s.\nTRAPPIST-1 is so dim you can\u0026rsquo;t see it without a strong telescope.\nTwo things are interesting about our findings on exoplanets: it is pretty common for a star to have planets (just a few decades ago this wasn\u0026rsquo;t an accepted reality); and our solar system seems to be a bit of an outlier in terms of the configuration of planets around the star based on what we\u0026rsquo;ve seen so far. A lot of these systems have Jupiter size or larger planets in very close orbit around the star; in the case of TRAPPIST-1, the star is much much weaker than the sun and all the planets we\u0026rsquo;ve found are in a very close tight orbit. This later fact may be due to selection bias, i.e. it is easier to find planets around a star when the planets are large and very close because the motion of the star is more strongly affected (for Doppler-effect discovered planets and also the luminosity detection method) and also when they are closer to the star and the plane of orbit is nearly head on relative to Earth. Also, of course, an Earth sized planet is going to cause a bigger dip in the intensity of light of a dwarf star when passing in between our telescopes and that star than would a similar size planet, say ours, have on a larger star like our sun (which itself isn\u0026rsquo;t all that big compared to some of the giant stars that can be found in the universe, but is substantially bigger than TRAPPIST-1). So right now the limitations of our detection technology is creating a statistical bias towards systems for which it is better able to detect exoplanets. As the next generation observation equipment is developed, we may start finding more solar systems which resemble ours.\nWhy we are excited about this discovery Three of the planets are believed to be capable of hosting liquid water oceans. Having liquid water oceans vastly increases the chances a planet could host life. Ultracool stars, which TRAPPIST-1 is classified as, are the most common type of stars in our galaxy. The fact that TRAPPIST-1 has so many planets and so many planets that are within the potentially habitable zone bodes well for our chances of finding life-hosting exoplanets if the negatives in the next section aren\u0026rsquo;t enough to stop life from evolving. Because the stars are so small, it is easier to see the dip in light which an Earth-sized planet would cause if it passes between us and TRAPPIST-1. TRAPPIST-1\u0026rsquo;s solar system\u0026rsquo;s orbital plane happens to be position just right for us to see these planets crossing the star; random chance says that only a ver small percentage of solar systems would yield this lucky result. However, because ultracool stars are so common, the odds are in our favor that we will continue to discover similar systems. Why we shouldn\u0026rsquo;t be too excited about this discovery The planets are probably tidally locked or near tidally locked and suffer run-away green house effects. One side of planet always faces the sun, the other always faces dark space. The resulting temperature differential would result in massive global winds if the planets retain their atmospheres. There is still a lot of debate about where the habitable zone is for tidally locked systems, and the predictions that three of these planets exist in the \u0026ldquo;Goldilocks zone\u0026rdquo; are not settled science.\nAs the director of the Carl Sagan Institute, Lisa Kaltenegger, points out, these planets are incredibly close to their star and their star could be emitting intense levels of ultraviolet light. UV radiation is dangerous for life as we know it, so in order to host light the planets would need some very serious ozone protection. Alternatively, assuming they had oceans, and that their oceans didn\u0026rsquo;t boil away or freeze over, life would probably be limited to deep oceans.\nUltracool dwarfs are often violent stars with massive flares. However, TRAPPIST-1 is relatively peaceful, according to some of the reports. [UPDATE AUG 10 2021]: See also the above update which suggests that these stars may not be as deadly via flares as originally thought.\nThey may possibly have lost their atmospheres due to being so close to the star.\nIt is only about 500 million years old, so there hasn\u0026rsquo;t been much time for life to evolve even if the above issues didn\u0026rsquo;t prevent said evolution. These stars, unlike ours, can hypothetically last for trillions of years since they burn their hydrogen fuel much more slowly than larger stars:\nEven if some of oceans remain and are frozen over, that doesn\u0026rsquo;t rule out life. Some of the moons in our solar system have frozen oceans of water where there is a suspected liquid layer underneath that could possible host life (e.g. Europa) which is why NASA has an interest in looking for potential life on Europa. So the conditions of the TRAPPIST-1 system may support some form of life but like with Europa, the ability for complex life to develop is highly limited due to lack of resources. Also, life can be fragile, and Earth went through many periods of mass extinctions; if any of the planets around TRAPPIST-1 have life, they are probably relegated to small regions in the twilight zones (that is, if one side always faces the star and the other always faces empty space, the part of the planet between these two zones would be in permanent twilight and have the mildest climates if the planets still have their atmospheres) making life on those planets much more vulnerable to total extinction, a fate Earth avoided in part due to being almost entirely habitable and thus having a more diverse biosphere.\nSobering Conclusions Forty lightyears is a long way away. Apollo 10\u0026rsquo;s max speed was around 11082.5 m/s which is the fastest manned craft speed we\u0026rsquo;ve ever obtained. That\u0026rsquo;s slower than how fast TRAPPIST-1 is moving toward us. 40 light years is 3.784e+17 meters away. If Apollo 10 went directly towards TRAPPIST-1 at its maximum obtained speed, it would close the distance at a rate of about 67082.5 m/s (the sum of its speed relative to Earth and the speed of TRAPPIST-1 relative to Earth using Galilean Relativity, these speeds are too slow to justify using Einstein\u0026rsquo;s Relativity). It would take Apollo 10 approximately 5.6408154e+12 seconds to make it to TRAPPIST-1. That\u0026rsquo;s nearly 178,846 years. Rocket technology hasn\u0026rsquo;t changed radically since then. And, of course, the astronauts wouldn\u0026rsquo;t live long enough to get even a tiny fraction of the way there.\nEven if the entire world turned all of their defense spending towards producing a Generation ship to reach TRAPPIST-1 and impoverished the planet\u0026rsquo;s resources to supply fuel and food supplies for said ship, we are still talking many tens of thousands of years at best, ignoring all threats to human life such as radiation, collision with stray objects, etc. One could easily imagine that within a few generations, the community on the generational ship would lose faith in their great-grandparent\u0026rsquo;s mission and turn the ship around, assuming they had enough fuel to do so. It is likely they would, as they would have to have some breaking capacity once they reach TRAPPIST-1, and then enough fuel to escape TRAPPIST-1\u0026rsquo;s gravity well. All to say that this is never going to happen.\nScience fiction has sparked our imagination of traveling to the stars, but the cold reality is that while there is probably a lot of life off Earth in this Universe, everybody is trapped in their own solar systems by relativity and the vast distances between stars. Also, Earth is about 4.5 billion years old and we have about another few billion years until our sun eats its inner planets; who knows what can happen in those billions of years. Keep in mind that human-level intelligence is probably very rare in any case. Some scientist have proposed based on genetic evidence that at one point about 75,000 years ago the human species was once reduced to 3,000 - 10,000 individuals due to climate catastrophes. If this is true (and not all scientist think it is), then we were once very close to extinction. Earth was very close to losing its human population, becoming a wild planet whose greatest intelligence might be the non-human apes that survive today. Not only must a planet have the right conditions for life, it must also have the right conditions for complex life, and even then, it\u0026rsquo;s going to take some luck to get to a human-like intelligence.\nSo we (complex intelligent life) are probably not alone but we are also probably very, very rare, and we likely won\u0026rsquo;t meet any alien intelligence before we go extinct.\nCoda In regards to the discussion above on generation ships, there is something related to that known as the Wait Calculation (A. Kennedy, Interstellar Travel: The Wait Calculation and the Incentive Trap of Progress, JBIS 59, July 2006) which is relevant beyond the concept of a generation ship.\nFrom the abstract:\nThis paper describes an incentive trap of growth that shows that civilisations may delay interstellar exploration as long as voyagers have the reasonable expectation that whenever they set out growth will continue to progress and find quicker means of travel, overtaking them to reach and colonise the destination before they do. This paper analyses the voyagers\u0026rsquo; wait calculation, using the example of a trip to Barnard\u0026rsquo;s Star, and finds a surprising minimum to time to destination at a given rate of growth that affects the expansion of all civilisations. Using simple equations of growth, it can be shown that there is a time where the negative incentive to travel turns positive and where departures will beat departures made at all other times. Waiting for fear future technology will make a journey redundant is irrational since it can be shown that if growth rates alter then leaving earlier may be a better option. It considers that while growth is resilient and may follow surprising avenues, a future discovery producing a quantum leap in travel technology that justifies waiting is unlikely.\nKennedy derives a Wait Equation: \u0026ldquo;The wait equation describes \u0026hellip; the point at which the negative incentive to leave changes to a positive one; where the incentive to set out on the interstellar journey is the strongest.\u0026rdquo;\nThe equation assumes that the growth rate of the speed of the potential ship per year is tied to the general global GDP growth rate:\n$$ v(t) = v_0 (1 + r)^t$$\nwhere $v$ is the velocity $t$ years after the velocity is $v_0$ and $r$ is the average growth rate year over year. Since velocity is inversely proportional to the time it would take to reach the star, we can rewrite this as\n$$ \\frac{T_0}{T} = (1+r)^t $$\nwhere $T$ is the time it would take if we waited $t$ years for technology to evolve, and $T_0$ is the time to take if we were to launch with today\u0026rsquo;s current technology. The purpose of the Wait Equation is to find the sweet spot for launching such that to wait any longer would not return any advantage.\nWe seek, then, the condition for which:\n$$ t + \\frac{T_0}{(1+r)^t} $$\nreaches a minimum. Using our previous rough estimate of 178,846 years for Apollo 10 to reach TRAPPIST-1 and a growth rate of 1%, we see that this minimum would happen about seven centuries from now.\nThe implication, if you accept the premises of this equation, is that to ensure that the generation ship we launch won\u0026rsquo;t be beat to the destination by a faster model, we should wait about 700 years to launch.\nA word of warning, of course. This model is entirely unrealistic. It would predict that we exceed the speed of light with our technology in a little over 1000 years:\nSo let\u0026rsquo;s in no way see this as a valid physical prediction. Instead, let\u0026rsquo;s see it as sort of a conceptual starting point for a discussion about \u0026ldquo;the incentive trap of progress.\u0026rdquo; I\u0026rsquo;ll revisit the Wait Equation again in some future blog post (probably) because it has applications elsewhere. For example, it might help us understand why it may be better to avoid getting a first or even second or third generation product. If you buy a first generation product and you don\u0026rsquo;t have a budget such that you can buy an upgraded product for many years ahead, then you will suffer a poorer quality product than you might if you waited a year or two for the next iterations of that product to improve. However, if you wait too long, you also harm yourself by denying yourself access to that product, and waiting too long means that the net harm to yourself could exceed the harm of buying too early.\nIndeed, that\u0026rsquo;s a potentially awesome post so let\u0026rsquo;s plan to discuss it again soon.\n","date":"2017-02-24T19:21:05-07:00","permalink":"https://tdj28.github.io/p/trappist-1-is-probably-not-alive/","title":"TRAPPIST-1 is probably not alive"},{"content":"Quick overview If interested in creating special effects for film projects. Uses Final Cut Pro X and CoreMelt\u0026rsquo;s TrackX Plugin\u0026rsquo;s Simple Tracker. In about thirty minutes you can create a passable video that portrays the Star War\u0026rsquo;s Death Star over Philadelphia, PA USA. Resulting video: Gathering the Software I\u0026rsquo;ll begin with an apology that the software discussed here requires Apple Mac* hardware to run. What we do here can be done with Adobe Premier Pro right out of the box.\nAssuming you have a Mac* device, this project also requires Final Cut Pro. This is not cheap software, but I\u0026rsquo;ve found it to be worth it if you plan to be doing enough projects. I have many nieces and nephews and we\u0026rsquo;ve had an awesome time making movies with this tool.\nUsing Keyframing, we could get a realistic tracking effect, but this would be a very tedious and time consuming way to proceed. Fortunately there is a plugin we can use that does the tracking nearly automatically.\nCoremelt\u0026rsquo;s TrackX Plugin powered by Mocha analyzes your video frame by frame and attempts to match the motion of the camera. The user needs only provide some very simple guidance.\nGathering the Media A quick image search will yield a wide range of choices for images of a space station, in this case the Death Star, but keep an eye towards an example that would look realistic in the context of your video. I chose an image that looked a lot like what the moon looks like in the daytime.\nI also filmed a couple of scenes around Philadelphia, being careful to move the camera enough to make it seem realistic, but smoothly enough with no quick motions to make tracking easier for myself.\nPreparing the Death Star image When you apply the image, however, you also must keep in mind that if it doesn\u0026rsquo;t match the shadowing seen in the video, the viewer\u0026rsquo;s mind will feel a bit off about it. Look for the shadows in your video and flip the Death Star image to match that direction using the flipped effect:\nThat is, import the media, drag it down to your timeline, and then drag the flipped effect to the corresponding timeline entry of the Death Star image. Next right click on that clip and create a new compound clip:\nThis is required for the TrackX plugin. Once you\u0026rsquo;ve done that, the compound clip of the Death Star will show up in your media and you can delete from your time line.\nBecause I had one scene facing West and another facing East, which is not recommended because that would imply two Death Stars in the sky, I needed to create two compound clips, one flipped from the other, in order to match the sun direction.\nTracking your video clips Next we can start the tracking process. Import your video clips from your device or hard-drive and then drag the corresponding file from the media section onto the timeline.\nThen, go to the generators menu and select Simple Tracker under the C2 TrackX entry:\nDrag that on top of the video clip. Your timeline will look something like this:\nAbove, I have three video clips and I have trimmed the Simple Tracker to fit each one and thus have three Simple Tracker entries in the timeline, one for each video.\nClick on the Simple Tracker entry on the timeline and you\u0026rsquo;ll now be able to set up tracking for your video:\nFiguring out what shape and placement for your area selection in Simple Tracker is an art more or less. Be sure to select an area that is completely in view for the majority of the clip. In the above screenshot, I chose the top of two sky-scrapers that are clearly and consistently within view during the entire clip. This helps the tracker maintain a passable tracking.\nDrag your compound clip of the Death Star that you made in the previous step over to the Insert Layer section, and then adjust the X, Y offsets and scale until the image placement in your video looks satisfactory.\nIn the timeline, move to the start of the video, and after you set your tracking shape selection, click on the Track forward button (white sideways arrow with the T in it). The plugin will iterate through each frame attempting to track the camera location in such a way that it can superimpose an image of the Death Star on the sky so that it appears as if it is stationary relative to the camera. In the resulting video (above) you can see, if you look closely enough, some flaws, but for the relatively small time put into it, the effect is quite good.\nTutorial Video Finally, here is a quick how-to video walking you through the process in real-time:\nFurther references Another example of this plugin\nUsing Adobe to create this effect\n","date":"2017-02-20T19:21:05-07:00","permalink":"https://tdj28.github.io/p/putting-a-space-station-over-philly-with-final-cut-pro/","title":"Putting a Space Station Over Philly With Final Cut Pro"},{"content":"Introduction Yet another chef tutorial You can of course skip this step if you are familiar with Chef to a great enough degree, but in the spirit of making this a self-contained walkthrough, we will start from the ground up.\nGetting started Installing ChefDK It is widely considered best practice to use ChefDK, and this walkthrough uses ChefDK exclusively.\nOn Mac, install HomeBrew and run brew cask install chefdk On Linux, download a package corresponding to your distro and install accordingly. On Windows: Run Linux as a Virtual Machine, get Docker for Windows, or try using Bash on Windows, but this walkthrough doesn\u0026rsquo;t support Windows-based development. Initiating your cookbook Let\u0026rsquo;s start with the following command:\n1 chef generate cookbook c9_ide_chef This command generates a template for a cookbook. Note that we use and underline in the name instead of a hyphen as hyphens can be problematic for chef in some circumstances. We now have a folder with the following content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ├── .gitignore ├── .kitchen.yml ├── Berksfile ├── README.md ├── chefignore ├── metadata.rb ├── recipes │ └── default.rb ├── spec │ ├── spec_helper.rb │ └── unit │ └── recipes │ └── default_spec.rb └── test └── smoke └── default └── default_test.rb It also creates a .delivery and a .git folder whose contents are omitted here for clarity.\nThe second command creates an attribute folder which we can populate with default attributes for our cookbook, and the third line creates a default template for a \u0026ldquo;message of the day\u0026rdquo; to be displayed when first logging in.\nThe .git directory is useful and we will use it as the basis for a new repo in github repo.\nPushing to GitHub Now that we have the skeleton for our cookbook, we should start pushing to GitHub for version-control. On GitHub, create a public repo called c9_ide. Then run the following commands:\n1 2 3 4 git add . git commit -m \u0026#34;First commit\u0026#34; git remote add origin git@github.com:YOUR_GITHUB_USERNAME/c9_ide_chef.git git push -u origin master Now the stage is set and we are ready to make the cookbook actually do something.\nCreate the core configuration Creating a story for the cookbook So far we\u0026rsquo;ve seen how to set up the cookbook, but we don\u0026rsquo;t quite know what we are going to do with it. Now is a good time to set up some goals for our cookbook:\nIt should update all packages before doing anything else It should create a message of the day that gives basic information about the server to a user logging in It should install basic security packages and other utilities It should install the packages needed to run Cloud 9 IDE It should pull and install Cloud 9 IDE Updating all packages Just for fun, we are going to make our cookbook work for both Debian and RHEL variations of Linux. Under the recipes folder, let\u0026rsquo;s create a recipe file called update.rb and give it contents:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Update system to start with case node[\u0026#39;platform\u0026#39;] when \u0026#39;debian\u0026#39;, \u0026#39;ubuntu\u0026#39; execute \u0026#39;AptUpdateUpgrade\u0026#39; do command \u0026#34;apt-get update \u0026amp;\u0026amp; DEBIAN_FRONTEND=noninteractive apt-get upgrade -y\u0026#34; end when \u0026#39;redhat\u0026#39;, \u0026#39;centos\u0026#39;, \u0026#39;fedora\u0026#39; # Update all packages execute \u0026#39;UpdateYum\u0026#39; do command \u0026#34;yum -y update\u0026#34; end # Add Fedora\u0026#39;s Extra Packages for Enterprise Linux execute \u0026#39;GetEPELRepo\u0026#39; do command \u0026#34;rpm -iUvh http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-9.noarch.rpm\u0026#34; not_if \u0026#34;rpm -qa | grep -qx \u0026#39;epel-release-7-9.noarch\u0026#39;\u0026#34; end end in the default.rb recipe, add the line:\n1 include_recipe \u0026#39;c9_ide::update\u0026#39; That\u0026rsquo;s it for now! Before we go any further, let\u0026rsquo;s test this. Chef has automatically added some basis for testing to your repo. Let\u0026rsquo;s start with kitchen. This assumes you have VirtualBox and Vagrant installed (if you don\u0026rsquo;t, install them). At the base directory, run:\n1 2 kitchen create kitchen converge The first command creates the virtual machines for us to work with, and teh second command cooks Chef inside of them. It is a very convenient way to do a full test of your code before pushing to github. The converge command may take quite a while as we are updating all packages on the systems and doing it for two VMs, and Ubuntu one and a CentOS one. If all goes well, the final output looks like:\n1 Chef Client finished, 1/1 resources updated in 02 minutes 21 seconds Creating a C9 User It is rarely a good idea to run a program as root. As such, our next step is to create a user for running the Cloud 9 IDE. We start by creating a data bag. First, install knife-solo and knife-solo data bag:\n1 2 chef gem install knife-solo chef gem install knife-solo_data_bag This is not entirely necessary, but that tool can be useful in other ways for making chef solo cookbooks. Then we can generate a databag and databag item:\n1 2 3 mkdir data_bags export EDITOR=\u0026#34;vi\u0026#34; knife solo data bag create users c9ide In the editor that pops up, paste the following content:\n1 2 3 4 5 6 7 8 9 { \u0026#34;id\u0026#34;: \u0026#34;c9ide\u0026#34;, \u0026#34;comment\u0026#34;: \u0026#34;c9ide User\u0026#34;, \u0026#34;ssh_keys\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;home\u0026#34;: \u0026#34;/home/c9ide\u0026#34;, \u0026#34;ssh_keygen\u0026#34;: false } Next, back to recipe/default.rb, we can invoke the external chef recipe \u0026ldquo;users\u0026rdquo; to have this user installed on our system:\n1 include_recipe \u0026#39;user::data_bag\u0026#39; To create an attributes file:\n1 chef generate attribute default and in attributes/default.rb add the line:\n1 default[\u0026#39;users\u0026#39;] = \u0026#39;c9ide\u0026#39; This of course shows the fantastic power and utility of Chef since we can call upon a wide range of libraries already created by the chef community. Now we need to tell Chef that we are using an external library, so in our metadata.rb file at the top level we add the line at the end:\n1 depends \u0026#39;user\u0026#39; At the same level there is a Berksfile with the content:\n1 2 3 source \u0026#39;https://supermarket.chef.io\u0026#39; metadata which tells chef to check this metadata.rb file for dependencies (such as the user dependency we just added) that we need to pull and make available during cooking. We\u0026rsquo;ll show how this is done manually a bit later, but for now kitchen converge takes care of that.\nNow run kitchen converge one more time, and you should see that the user is created.\nLet\u0026rsquo;s create a quick smoke test item, add:\n1 2 3 describe user(\u0026#39;c9ide\u0026#39;) do it { should exist } end to test/default/default_test.rb and run\n1 kitchen verify if all went well you will see:\n1 2 User c9ide ✔ should exist Now is a good time to push to github:\n1 2 3 git add . git commit -m \u0026#34;Adds update and users\u0026#34; git push origin master Installing MOTD First, let\u0026rsquo;s generate a blank template file:\n1 chef generate template motd fill with content:\n1 2 3 \u0026lt;%= node[\u0026#39;motd-attributes\u0026#39;][\u0026#39;message\u0026#39;] %\u0026gt; The hostname of this node is \u0026lt;%= node[\u0026#39;hostname\u0026#39;] %\u0026gt; The IP address of this node is \u0026lt;%= node[\u0026#39;ipaddress\u0026#39;] %\u0026gt; and create a recipe file recipes/motd.rb with contents:\n1 2 3 4 template \u0026#39;/etc/motd\u0026#39; do source \u0026#39;motd.erb\u0026#39; mode \u0026#39;0644\u0026#39; end and finally add this line to the attributes/default.rb file:\n1 default[\u0026#39;motd-attributes\u0026#39;][\u0026#39;message\u0026#39;] = \u0026#34;A Cloud 9 IDE server\u0026#34; We add the following tests at test/smoke/default/motd_test.rb:\n1 2 3 describe directory(\u0026#39;/etc/motd\u0026#39;) do # describe this directory its(:content) { should match /A Cloud 9 IDE server/ } end We run kitchen converge and kitchen verify again to make sure the recipe is fine, and then push to github as before.\nInstalling Packages We are getting close to installing the Cloud9 IDE. Before doing so, we need to install some packages. Some of these packages are for security purposes, or convenience in administration, while others are needed for the Cloud 9 IDE system.\nCreate a file recipe/packages.rb with content:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 case node[\u0026#39;platform\u0026#39;] when \u0026#39;debian\u0026#39;, \u0026#39;ubuntu\u0026#39; req_packages = [\u0026#39;fail2ban\u0026#39;, \u0026#39;gcc\u0026#39;, \u0026#39;g++\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;htop\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;nodejs\u0026#39;, \u0026#39;nmap\u0026#39;, \u0026#39;npm\u0026#39;, \u0026#39;sysstat\u0026#39;, \u0026#39;unattended-upgrades\u0026#39;, \u0026#39;apt-listchanges\u0026#39;] req_packages.each do |packs| Chef::Log.info(\u0026#34;Installing: \u0026#34; + packs) package packs do action :install end end template \u0026#39;/etc/apt/apt.conf.d/50unattended-upgrades\u0026#39; do owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0644\u0026#39; source \u0026#39;50unattended-upgrades.erb\u0026#39; end template \u0026#39;/etc/apt/apt.conf.d/20auto-upgrades\u0026#39; do owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0644\u0026#39; source \u0026#39;20auto-upgrades.erb\u0026#39; end when \u0026#39;redhat\u0026#39;, \u0026#39;centos\u0026#39;, \u0026#39;fedora\u0026#39; req_packages = [\u0026#39;fail2ban\u0026#39;, \u0026#39;gcc-c++\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;glibc-static\u0026#39;, \u0026#39;htop\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;nodejs\u0026#39;, \u0026#39;nmap\u0026#39;, \u0026#39;npm\u0026#39;, \u0026#39;sysstat\u0026#39;, \u0026#39;yum-cron\u0026#39;] req_packages.each do |packs| Chef::Log.info(\u0026#34;Installing: \u0026#34; + packs) package packs do action :install end end template \u0026#39;/etc/yum/yum-cron.conf\u0026#39; do owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0644\u0026#39; source \u0026#39;yum-cron.conf.erb\u0026#39; end execute \u0026#39;systemctl enable yum-cron\u0026#39; do end execute \u0026#39;systemctl start yum-cron\u0026#39; do end end Notice that we have sneaked in automated security updates as well. For that, we require the following templates:\ntemplates/50unattended-upgrades.erb:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Automatically upgrade packages from these (origin, archive) pairs Unattended-Upgrade::Allowed-Origins { \u0026lt;% node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;origins\u0026#39;].each do |origin| %\u0026gt; \u0026#34;\u0026lt;%= origin %\u0026gt;\u0026#34;; \u0026lt;% end %\u0026gt; }; // List of packages to not update Unattended-Upgrade::Package-Blacklist { \u0026#34;apache2\u0026#34;; \u0026#34;nginx\u0026#34;; // \u0026#34;libc6-dev\u0026#34;; // \u0026#34;libc6-i686\u0026#34;; }; // Send email to this address for problems or packages upgrades // If empty or unset then no email is sent, make sure that you // have a working mail setup on your system. The package \u0026#39;mailx\u0026#39; // must be installed or anything that provides /usr/bin/mail. \u0026lt;%= node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;send_email\u0026#39;] ? \u0026#34;\u0026#34; : \u0026#34;// \u0026#34; %\u0026gt;Unattended-Upgrade::Mail \u0026#34;\u0026lt;%= node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;email_address\u0026#39;] %\u0026gt;\u0026#34;; // Do automatic removal of new unused dependencies after the upgrade // (equivalent to apt-get autoremove) Unattended-Upgrade::Remove-Unused-Dependencies \u0026#34;\u0026lt;%= node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;auto_remove\u0026#39;] ? \u0026#34;true\u0026#34; : \u0026#34;false\u0026#34; %\u0026gt;\u0026#34;; // Automatically reboot *WITHOUT CONFIRMATION* if a // the file /var/run/reboot-required is found after the upgrade Unattended-Upgrade::Automatic-Reboot \u0026#34;\u0026lt;%= node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;auto_reboot\u0026#39;] ? \u0026#34;true\u0026#34; : \u0026#34;false\u0026#34; %\u0026gt;\u0026#34;; templates/20auto-upgrades.erb:\n1 2 APT::Periodic::Update-Package-Lists \u0026#34;\u0026lt;%=node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;update_package_lists_interval\u0026#39;]%\u0026gt;\u0026#34;; APT::Periodic::Unattended-Upgrade \u0026#34;\u0026lt;%=node[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;upgrade_interval\u0026#39;]%\u0026gt;\u0026#34;; and\ntemplates/yum-cron.conf.erb:\n1 2 update_cmd = security apply_updates = yes 1 2 3 4 5 6 7 default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;update_package_lists_interval\u0026#39;] = \u0026#34;1\u0026#34; default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;upgrade_interval\u0026#39;] = \u0026#34;1\u0026#34; default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;origins\u0026#39;] = [\u0026#39;${distro_id} ${distro_codename}-security\u0026#39;] default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;send_email\u0026#39;] = false default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;email_address\u0026#39;] = \u0026#34;test@example.com\u0026#34; default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;auto_remove\u0026#39;] = false default[\u0026#39;unattended-upgrades\u0026#39;][\u0026#39;auto_reboot\u0026#39;] = false and of course, our inspec test in test/smoke/default/packages_test.rb:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 if os[:family] == \u0026#39;debian\u0026#39; req_packages = [\u0026#39;fail2ban\u0026#39;, \u0026#39;gcc\u0026#39;, \u0026#39;g++\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;htop\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;nodejs\u0026#39;, \u0026#39;nmap\u0026#39;, \u0026#39;npm\u0026#39;, \u0026#39;sysstat\u0026#39;, \u0026#39;unattended-upgrades\u0026#39;, \u0026#39;apt-listchanges\u0026#39;] req_packages.each do |packs| describe package(packs) do it { should be_installed } end end else req_packages = [\u0026#39;fail2ban\u0026#39;, \u0026#39;gcc-c++\u0026#39;, \u0026#39;git\u0026#39;, \u0026#39;glibc-static\u0026#39;, \u0026#39;htop\u0026#39;, \u0026#39;make\u0026#39;, \u0026#39;nodejs\u0026#39;, \u0026#39;nmap\u0026#39;, \u0026#39;npm\u0026#39;, \u0026#39;sysstat\u0026#39;, \u0026#39;yum-cron\u0026#39;] req_packages.each do |packs| describe package(packs) do it { should be_installed } end end end Installing Cloud 9 IDE Chef has a built-in git resource that we can use to pull the Cloud 9 IDE code.\nThe test for this section will be rather brief:\n1 2 3 describe directory(\u0026#39;/home/c9ide/core/README.md\u0026#39;) do # describe this directory its(:content) { should match /^Cloud9/ } end and the reason is that we can really verify whether or not this section succeeded in the next section on running via supervisor.\nThe code for this section, in /recipes/c9ide.rb, is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 git \u0026#34;/home/c9ide/core\u0026#34; do repository \u0026#39;https://github.com/c9/core.git\u0026#39; reference \u0026#39;master\u0026#39; action :sync end case node[\u0026#39;platform\u0026#39;] when \u0026#39;debian\u0026#39;, \u0026#39;ubuntu\u0026#39; bash \u0026#39;create nodejs link\u0026#39; do code \u0026lt;\u0026lt;-EOH ln -s /usr/bin/nodejs /usr/bin/node EOH not_if { ::File.exist?(\u0026#39;/usr/bin/node\u0026#39;) } end end bash \u0026#39;install_cloud9ide\u0026#39; do cwd \u0026#39;/home/c9ide/core/\u0026#39; user \u0026#39;root\u0026#39; code \u0026lt;\u0026lt;-EOH scripts/install-sdk.sh EOH environment \u0026#39;PREFIX\u0026#39; =\u0026gt; \u0026#39;/usr/local\u0026#39; end Installing Supervisor Finally, we want to leave our server running Cloud 9 IDE after provisioning it. Supervisor is a fantastic way to do so, though of course not the only way and perhaps not even the best way, but it is a convenient and production-ready solution (though this development kit version of Cloud 9 IDE is not production material and is meant for development purposes).\nIn recipes/supervisor.rb:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package \u0026#39;supervisor\u0026#39; do action :install end case node[\u0026#39;platform\u0026#39;] when \u0026#39;debian\u0026#39;, \u0026#39;ubuntu\u0026#39; cookbook_file \u0026#39;/etc/supervisor/conf.d/cloud9ide.conf\u0026#39; do source \u0026#39;c9ide.conf\u0026#39; owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0755\u0026#39; action :create end execute \u0026#39;start supervisor\u0026#39; do command \u0026#39;service supervisor start\u0026#39; end execute \u0026#39;reload supervisor\u0026#39; do command \u0026#39;supervisorctl update\u0026#39; end when \u0026#39;redhat\u0026#39;, \u0026#39;centos\u0026#39;, \u0026#39;fedora\u0026#39; cookbook_file \u0026#39;/etc/supervisord.d/cloud9ide.ini\u0026#39; do source \u0026#39;c9ide.conf\u0026#39; owner \u0026#39;root\u0026#39; group \u0026#39;root\u0026#39; mode \u0026#39;0755\u0026#39; action :create end execute \u0026#39;start supervisor\u0026#39; do command \u0026#39;service supervisord start\u0026#39; end execute \u0026#39;reload supervisor\u0026#39; do command \u0026#39;supervisorctl update\u0026#39; end end and finally add this to the c9ide_test.rb test file:\n1 2 3 describe port(9999) do it { should be_listening } end Putting it all together The default recipe should now look like this:\n1 2 3 4 5 6 include_recipe \u0026#39;c9_ide::update\u0026#39; include_recipe \u0026#39;user::data_bag\u0026#39; include_recipe \u0026#39;c9_ide::motd\u0026#39; include_recipe \u0026#39;c9_ide::packages\u0026#39; include_recipe \u0026#39;c9_ide::cloud9ide\u0026#39; include_recipe \u0026#39;c9_ide::supervisor\u0026#39; we run a final kitchen converge; if all goes well, we follow up with a final kitchen verify:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 -----\u0026gt; Starting Kitchen (v1.14.2) -----\u0026gt; Setting up \u0026lt;default-ubuntu-1604\u0026gt;... Finished setting up \u0026lt;default-ubuntu-1604\u0026gt; (0m0.00s). -----\u0026gt; Verifying \u0026lt;default-ubuntu-1604\u0026gt;... Loaded Target: ssh://vagrant@127.0.0.1:2222 File /home/c9ide/core/README.md ✔ content should match /^Cloud9/ Port 9999 ✔ should be listening User c9ide ✔ should exist File /etc/motd ✔ content should match /A Cloud 9 IDE server/ System Package ✔ fail2ban should be installed System Package ✔ gcc should be installed System Package ✔ g++ should be installed System Package ✔ git should be installed System Package ✔ htop should be installed System Package ✔ make should be installed System Package ✔ nodejs should be installed System Package ✔ nmap should be installed System Package ✔ npm should be installed System Package ✔ sysstat should be installed System Package ✔ unattended-upgrades should be installed System Package ✔ apt-listchanges should be installed Test Summary: 16 successful, 0 failures, 0 skipped Finished verifying \u0026lt;default-ubuntu-1604\u0026gt; (0m1.28s). -----\u0026gt; Setting up \u0026lt;default-centos-72\u0026gt;... Finished setting up \u0026lt;default-centos-72\u0026gt; (0m0.00s). -----\u0026gt; Verifying \u0026lt;default-centos-72\u0026gt;... Loaded Target: ssh://vagrant@127.0.0.1:2200 File /home/c9ide/core/README.md ✔ content should match /^Cloud9/ Port 9999 ✔ should be listening User c9ide ✔ should exist File /etc/motd ✔ content should match /A Cloud 9 IDE server/ System Package ✔ fail2ban should be installed System Package ✔ gcc-c++ should be installed System Package ✔ git should be installed System Package ✔ glibc-static should be installed System Package ✔ htop should be installed System Package ✔ make should be installed System Package ✔ nodejs should be installed System Package ✔ nmap should be installed System Package ✔ npm should be installed System Package ✔ sysstat should be installed System Package ✔ yum-cron should be installed Test Summary: 15 successful, 0 failures, 0 skipped Finished verifying \u0026lt;default-centos-72\u0026gt; (0m13.80s). -----\u0026gt; Kitchen is finished. (0m16.47s) Congratulations, you have a basic working chef recipe for installing Cloud 9 IDE, tested and provisioned on both major families of Linux.\nSource Code You can find the source code for this recipe on my github account here.\nFuture Steps We took some shortcuts and avoided some best practices for convenience here. Ideally we would like some unit tests and to pull in more community recipes. For example, the supervisor chef cookbook could be used here instead of dealing with that in a platform case.\nIn the forthcoming Part II, we will use Puppet, a widely used alternative to Chef, to provision a server that runs the Cloud 9 IDE SDK as we have here with Chef.\n","date":"2017-01-28T19:21:05-07:00","permalink":"https://tdj28.github.io/p/devops-101-chef-cookbook-with-testing/","title":"DevOps 101: Chef Cookbook with Testing"},{"content":"A Python feature? A colleague noticed this behavior:\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; in \u0026#39;b\u0026#39; == 0 False \u0026gt;\u0026gt;\u0026gt; (\u0026#39;a\u0026#39; in \u0026#39;b\u0026#39;) == 0 True \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; in (\u0026#39;b\u0026#39; == 0) TypeError: argument of type \u0026#39;bool\u0026#39; is not iterable offering a bucket of doubloons to anyone who can explain it. The final case is simply a typecasting problem, but the first two cases demonstrate Python\u0026rsquo;s operating chaining behavior.\nExplanation Using the Dis Module to dig deeper Using the dis1 module, we find:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 \u0026#39;a\u0026#39; in \u0026#39;b\u0026#39; == 0 7 0 LOAD_CONST 1 (\u0026#39;a\u0026#39;) 3 LOAD_CONST 2 (\u0026#39;b\u0026#39;) 6 DUP_TOP 7 ROT_THREE 8 COMPARE_OP 6 (in) 11 JUMP_IF_FALSE_OR_POP 21 14 LOAD_CONST 3 (0) 17 COMPARE_OP 2 (==) 20 RETURN_VALUE \u0026gt;\u0026gt; 21 ROT_TWO 22 POP_TOP 23 RETURN_VALUE \u0026#39;a\u0026#39; in \u0026#39;b\u0026#39; and \u0026#39;b\u0026#39; == 0 15 0 LOAD_CONST 1 (\u0026#39;a\u0026#39;) 3 LOAD_CONST 2 (\u0026#39;b\u0026#39;) 6 COMPARE_OP 6 (in) 9 JUMP_IF_FALSE_OR_POP 21 12 LOAD_CONST 2 (\u0026#39;b\u0026#39;) 15 LOAD_CONST 3 (0) 18 COMPARE_OP 2 (==) \u0026gt;\u0026gt; 21 RETURN_VALUE \u0026#39;a\u0026#39; in \u0026#39;a\u0026#39; == 0 9 0 LOAD_CONST 1 (\u0026#39;a\u0026#39;) 3 LOAD_CONST 1 (\u0026#39;a\u0026#39;) 6 DUP_TOP 7 ROT_THREE 8 COMPARE_OP 6 (in) 11 JUMP_IF_FALSE_OR_POP 21 14 LOAD_CONST 2 (0) 17 COMPARE_OP 2 (==) 20 RETURN_VALUE \u0026gt;\u0026gt; 21 ROT_TWO 22 POP_TOP 23 RETURN_VALUE In words Parenthesis essential forces Python to do \u0026lsquo;something\u0026rsquo; first before doing anything else. I often use parenthesis out of habit even when not needed just to be 100% clear. The above behavior shows where that habit might come in handy if Python\u0026rsquo;s operation chaining isn\u0026rsquo;t the desired behavior. When we put 'a' in 'b' inside parenthesis, Python is forced to evaluate that independently of the rest of line; this evaluates to True or False, and then the == 0 comparison acts as a Boolean comparison, giving expected behavior since Python sees False and 0 as equivalent here.\nIn the first instance above, however, Python is operator chaining. For 'a' in 'b' == 0, it first loads a and b strings such that the stack is\n1 [ b, a ] it then DUP_TOP duplicates b so that it doesn’t have to waste time to load it again for the second evaluation, resulting in the stack:\n1 [ b, b, a ] ROT_THREE lifts second and third stack item up and the top item down to third place, so now the stack is:\n1 [ b, a, b ] COMPARE_OP acts on [ b, a], and since it is not true, JUMP_IF_FALSE_OR_POP forces it to jump and just returns False. This makes sense because it sees this as a compound AND statement, and when the first part of an AND statement is false, the entire thing is false and it is a waste of resources to compute further.\nWhat is interesting is if you do 'a' in 'a' == 1, or 'a' in 'a' == 0. Take the former, for example, which after DUP_TOP gives us the stack:\n1 [ a, a, a ] Here 'a' in 'a' is obviously true, and these two entries of the stack are replaced with the result of that evaluation, which is True. This, now being the top of the stack, is popped off by JUMP_IF_FALSE_OR_POP leaving the duplicated \u0026lsquo;a\u0026rsquo; on the top of the stack, where then 1 is loaded for the next comparison, so the stack is now:\n1 [1, a] and the == comparison is applied which is indeed False. Thus 'a' in 'a' == 0 is chained as 'a' in 'a' and 'a' == 0 and will also be false. Similarly, 'a' in 'b' == 0 is effectively the same as 'a' in 'b' and 'b' == 0 which is false.\nThe following behavior, then, makes sense:\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; in \u0026#39;a\u0026#39; True \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; in \u0026#39;a\u0026#39; and \u0026#39;a\u0026#39; == True False \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; in \u0026#39;a\u0026#39; == True False pip install dis\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2017-01-20T19:21:05-07:00","permalink":"https://tdj28.github.io/p/python-uses-operational-chaining-for-boolean-comparisons/","title":"Python Uses Operational Chaining for Boolean Comparisons"},{"content":"Goal: Network Namespace is a Linux tool that allows for the easy virtualization of network models. While it has plenty of practical uses directly on the hardware, this will be a quick introduction to how it can be used to create more complex Docker networks that could be extended to modeling, say, the infrastructure of a data center or cloud provider.\nBrandon Rhodes created a great \u0026ldquo;playground\u0026rdquo; for his book Foundations of Python Network Programming. Inspired by this example, we will create a small simple netns/docker example that is more bite-sized for those who aren\u0026rsquo;t familiar with these tools.\nWhat is NetNS? Very briefly, netns is a tool that allows us to create virtual network namespaces that are isolated from each other.\nLet\u0026rsquo;s let its man page speak for itself:\nA network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.\nBy default a process inherits its network namespace from its parent.Initially all the processes share the same default network namespace from the init process.\nOne\u0026rsquo;s imagination can fly away at this point thinking of the multitude of possibilities this offers for security, but that should be a focus of a separate entry.\nCreating a new namespace, here let\u0026rsquo;s call it newtonsapple for the sake of example, is as easy as\n:::bash ip netns add newtonsapple Of course there is much, much more to it than that, but we will see more details about what netns can do below.\nWhat is Docker? Chances are pretty high that if you found this page, you have a pretty good idea of what Docker is. I will put it in the context of our netns discussion. In some ways, Docker is to the OS what netns is the machine\u0026rsquo;s networking configuration. Docker is a way to run processes in an isolated environment that does not use virtualization of hardware, but rather has direct access to the hardware via having direct access to the kernel. There are plenty of places out there that will provide fantastic introductions to Docker, but for the sake of this entry, it helps to think of netns and Docker as sort of cousins in that they both create isolated sandboxes which can interact in controlled ways with the real OS/network outside of their bubbles (in the case of Docker this bubble is called a container and in case of netns this bubble is called a namespace).\nDocker Networking Docker has a great tutorial on Docker Networking and it would be inefficient to recreate that here in any way. Instead I want to highlight the key parts that are relevant to this entry.\nThe first thing to note is that Docker automatically creates its own network. If you do an ifconfig or an ip a on a machine running a Docker server, you will find an entry corresponding to this network listed in the output:\n1 2 3 docker0 Link encap:Ethernet HWaddr 56:84:7a:fe:97:99 inet addr:172.17.42.1 Bcast:0.0.0.0 Mask:255.255.0.0 ... Docker also creates a network internal to each container (loopback) and you can launch a container that hooks directly in to the host network (docker run --net=host). The default is to launch it on the docker0 network, though it is often useful to run it on the host network if, for example, it needs access to VPN connections and it isn\u0026rsquo;t worth the effort to create further bridges. However, an interesting option is --net=none. This tells Docker to not touch the networking of the container and allows us to create our own networking for the containers.\nUsing NetNS with Docker to Model Your Home Network Preliminary setup This post will assume you are working from Ubuntu/Debian, though any Linux distribution will have these tools.\nOn Ubuntu, you have to create a directory /var/run/netns in order to use netns:\n1 sudo mkdir -p /var/run/netns Though this wasn\u0026rsquo;t my experience, you may have to enable two Linux kernel modules as well:\n1 sudo modprobe ip_nat_ftp nf_conntrack_ftp You may also need to install bridge-utils if it isn\u0026rsquo;t already installed:\n1 2 sudo apt-get update sudo apt-get install bridge-utils Bringing up the containers and linking netns 1 2 3 4 5 6 7 8 9 10 11 docker run --net=none --dns=8.8.8.8 --name=verizon -d ubuntu /bin/sh -c \u0026#34;while true; do echo \u0026#34;\u0026#34;; done\u0026#34; pid=$(docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; verizon) sudo ln -s /proc/$pid/ns/net /var/run/netns/verizon docker run --net=none --dns=8.8.8.8 --name=router -d ubuntu /bin/sh -c \u0026#34;while true; do echo \u0026#34;\u0026#34;; done\u0026#34; pid=$(docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; router) sudo ln -s /proc/$pid/ns/net /var/run/netns/router docker run --net=none --dns=8.8.8.8 --name=laptop -d ubuntu /bin/sh -c \u0026#34;while true; do echo \u0026#34;\u0026#34;; done\u0026#34; pid=$(docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; laptop) sudo ln -s /proc/$pid/ns/net /var/run/netns/laptop Creating Network Interfaces 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 root@vagrant-ubuntu-trusty-64:~/code# ip netns exec verizon ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 10: eth1: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether f6:8b:e9:66:00:b0 brd ff:ff:ff:ff:ff:ff root@vagrant-ubuntu-trusty-64:~/code# ip netns exec router ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 4: eth1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 36:7a:68:d8:a5:84 brd ff:ff:ff:ff:ff:ff 9: eth0: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc pfifo_fast state DOWN mode DEFAULT group default qlen 1000 link/ether fe:60:62:9d:1b:7b brd ff:ff:ff:ff:ff:ff root@vagrant-ubuntu-trusty-64:~/code# ip netns exec laptop ip link list 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 7: eth1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether f2:ee:9e:56:55:d4 brd ff:ff:ff:ff:ff:ff root@vagrant-ubuntu-trusty-64:~/code# brctl show bridge name bridge id STP enabled interfaces docker0 8000.56847afe9799 no home 8000.821c2640a1ea no laptop_eth1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ### Create network namespaces ip netns add levela ip netns add levelb ip netns add levelc ### Create peer devices ip link add veth0a type veth peer name veth1a ip link add veth0b type veth peer name veth1b ip link add veth0c type veth peer name veth1c ### Put these devices in namespaces # veth0a remains in globalspace ip link set veth1a netns levela ip link set veth0b netns levela ip link set veth1b netns levelb ip link set veth0c netns levelb ip link set veth1c netns levelc ### Set up Networks ip netns exec levela ifconfig veth1a 172.16.1.0/24 up ip netns exec levelb ifconfig veth1b 10.1.1.1/24 up ip netns exec levelc ifconfig veth1c 192.168.1.1/24 up At thist point we should be able to observer network namespaces and corresponding devices with their networks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # ip netns exec levela ip addr list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 6: veth1a: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 link/ether ba:8c:ec:6e:71:db brd ff:ff:ff:ff:ff:ff inet 172.16.1.0/24 brd 172.16.1.255 scope global veth1a valid_lft forever preferred_lft forever 9: veth0b: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 82:88:3d:09:e7:1f brd ff:ff:ff:ff:ff:ff # ip netns exec levelb ip addr list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 8: veth1b: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 link/ether c2:c4:9c:2b:41:a5 brd ff:ff:ff:ff:ff:ff inet 10.1.1.1/24 brd 10.1.1.255 scope global veth1b valid_lft forever preferred_lft forever 11: veth0c: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 96:a5:1c:5b:e3:8c brd ff:ff:ff:ff:ff:ff # ip netns exec levelc ip addr list 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 10: veth1c: \u0026lt;NO-CARRIER,BROADCAST,MULTICAST,UP\u0026gt; mtu 1500 qdisc pfifo_fast state DOWN group default qlen 1000 link/ether 66:d7:ff:9b:65:63 brd ff:ff:ff:ff:ff:ff inet 192.168.1.1/24 brd 192.168.1.255 scope global veth1c valid_lft forever preferred_lft forever and corresponding routes:\n1 2 3 4 5 6 7 8 9 10 11 # ip netns exec levela ip route list 172.16.1.0/24 dev veth1a proto kernel scope link src 172.16.1.0 # ip netns exec levelb ip route list 10.1.1.0/24 dev veth1b proto kernel scope link src 10.1.1.1 # ip netns exec levelc ip route list 192.168.1.0/24 dev veth1c proto kernel scope link src 192.168.1.1 ","date":"2016-03-03T19:21:05-07:00","permalink":"https://tdj28.github.io/p/abstracting-networking-with-docker-containers/","title":"Abstracting Networking with Docker Containers"}]